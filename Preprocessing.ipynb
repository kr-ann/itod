{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка: лемматизация и разметка ЧР по universal dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# используем udpipe (vs pymystem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ufal.udpipe in c:\\users\\boss\\anaconda3\\lib\\site-packages (1.2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install ufal.udpipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ufal.udpipe import Model, Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\boss\\anaconda3\\lib\\site-packages (3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfile = \"C:\\\\Users\\\\boss\\\\Documents\\\\Diploma\\\\RNC_Subcorpus\\\\udpipe_syntagrus.model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Скрипт для предобработки "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/akutuzov/webvectors/blob/master/preprocessing/rus_preprocessing_udpipe.py\n",
    "'''\n",
    "Этот скрипт принимает на вход необработанный русский текст \n",
    "(одно предложение на строку или один абзац на строку).\n",
    "Он токенизируется, лемматизируется и размечается по частям речи с использованием UDPipe.\n",
    "На выход подаётся последовательность разделенных пробелами лемм с частями речи \n",
    "(\"зеленый_NOUN трамвай_NOUN\").\n",
    "Их можно непосредственно использовать в моделях с RusVectōrēs (https://rusvectores.org).\n",
    "Примеры запуска:\n",
    "echo 'Мама мыла раму.' | python3 rus_preprocessing_udpipe.py\n",
    "zcat large_corpus.txt.gz | python3 rus_preprocessing_udpipe.py | gzip > processed_corpus.txt.gz\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from future import standard_library\n",
    "import sys\n",
    "import os\n",
    "import wget\n",
    "import re\n",
    "from ufal.udpipe import Model, Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вспомогательные функции для очистки текста\n",
    "\n",
    "def num_replace(word):\n",
    "    newtoken = 'x' * len(word)\n",
    "    return newtoken\n",
    "\n",
    "\n",
    "def clean_token(token, misc):\n",
    "    \"\"\"\n",
    "    :param token:  токен (строка)\n",
    "    :param misc:  содержимое поля \"MISC\" в CONLLU (строка)\n",
    "    :return: очищенный токен (строка)\n",
    "    \"\"\"\n",
    "    out_token = token.strip().replace(' ', '')\n",
    "    if token == 'Файл' and 'SpaceAfter=No' in misc:\n",
    "        return None\n",
    "    return out_token\n",
    "\n",
    "\n",
    "def clean_lemma(lemma, pos):\n",
    "    \"\"\"\n",
    "    :param lemma: лемма (строка)\n",
    "    :param pos: часть речи (строка)\n",
    "    :return: очищенная лемма (строка)\n",
    "    \"\"\"\n",
    "    out_lemma = lemma.strip().replace(' ', '').replace('_', '').lower()\n",
    "    if '|' in out_lemma or out_lemma.endswith('.jpg') or out_lemma.endswith('.png'):\n",
    "        return None\n",
    "    if pos != 'PUNCT':\n",
    "        if out_lemma.startswith('«') or out_lemma.startswith('»'):\n",
    "            out_lemma = ''.join(out_lemma[1:])\n",
    "        if out_lemma.endswith('«') or out_lemma.endswith('»'):\n",
    "            out_lemma = ''.join(out_lemma[:-1])\n",
    "        if out_lemma.endswith('!') or out_lemma.endswith('?') or out_lemma.endswith(',') \\\n",
    "                or out_lemma.endswith('.'):\n",
    "            out_lemma = ''.join(out_lemma[:-1])\n",
    "    return out_lemma\n",
    "\n",
    "\n",
    "def list_replace(search, replacement, text):\n",
    "    search = [el for el in search if el in text]\n",
    "    for c in search:\n",
    "        text = text.replace(c, replacement)\n",
    "    return text\n",
    "\n",
    "\n",
    "def unify_sym(text):  # принимает строку в юникоде\n",
    "    text = list_replace \\\n",
    "        ('\\u00AB\\u00BB\\u2039\\u203A\\u201E\\u201A\\u201C\\u201F\\u2018\\u201B\\u201D\\u2019', '\\u0022', text)\n",
    "\n",
    "    text = list_replace \\\n",
    "        ('\\u2012\\u2013\\u2014\\u2015\\u203E\\u0305\\u00AF', '\\u2003\\u002D\\u002D\\u2003', text)\n",
    "\n",
    "    text = list_replace('\\u2010\\u2011', '\\u002D', text)\n",
    "\n",
    "    text = list_replace \\\n",
    "            (\n",
    "            '\\u2000\\u2001\\u2002\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200A\\u200B\\u202F\\u205F\\u2060\\u3000',\n",
    "            '\\u2002', text)\n",
    "\n",
    "    text = re.sub('\\u2003\\u2003', '\\u2003', text)\n",
    "    text = re.sub('\\t\\t', '\\t', text)\n",
    "\n",
    "    text = list_replace \\\n",
    "            (\n",
    "            '\\u02CC\\u0307\\u0323\\u2022\\u2023\\u2043\\u204C\\u204D\\u2219\\u25E6\\u00B7\\u00D7\\u22C5\\u2219\\u2062',\n",
    "            '.', text)\n",
    "\n",
    "    text = list_replace('\\u2217', '\\u002A', text)\n",
    "\n",
    "    text = list_replace('…', '...', text)\n",
    "\n",
    "    text = list_replace('\\u2241\\u224B\\u2E2F\\u0483', '\\u223D', text)\n",
    "\n",
    "    text = list_replace('\\u00C4', 'A', text)  # латинская\n",
    "    text = list_replace('\\u00E4', 'a', text)\n",
    "    text = list_replace('\\u00CB', 'E', text)\n",
    "    text = list_replace('\\u00EB', 'e', text)\n",
    "    text = list_replace('\\u1E26', 'H', text)\n",
    "    text = list_replace('\\u1E27', 'h', text)\n",
    "    text = list_replace('\\u00CF', 'I', text)\n",
    "    text = list_replace('\\u00EF', 'i', text)\n",
    "    text = list_replace('\\u00D6', 'O', text)\n",
    "    text = list_replace('\\u00F6', 'o', text)\n",
    "    text = list_replace('\\u00DC', 'U', text)\n",
    "    text = list_replace('\\u00FC', 'u', text)\n",
    "    text = list_replace('\\u0178', 'Y', text)\n",
    "    text = list_replace('\\u00FF', 'y', text)\n",
    "    text = list_replace('\\u00DF', 's', text)\n",
    "    text = list_replace('\\u1E9E', 'S', text)\n",
    "\n",
    "    currencies = list \\\n",
    "            (\n",
    "            '\\u20BD\\u0024\\u00A3\\u20A4\\u20AC\\u20AA\\u2133\\u20BE\\u00A2\\u058F\\u0BF9\\u20BC\\u20A1\\u20A0\\u20B4\\u20A7\\u20B0\\u20BF\\u20A3\\u060B\\u0E3F\\u20A9\\u20B4\\u20B2\\u0192\\u20AB\\u00A5\\u20AD\\u20A1\\u20BA\\u20A6\\u20B1\\uFDFC\\u17DB\\u20B9\\u20A8\\u20B5\\u09F3\\u20B8\\u20AE\\u0192'\n",
    "        )\n",
    "\n",
    "    alphabet = list \\\n",
    "            (\n",
    "            '\\t\\n\\r абвгдеёзжийклмнопрстуфхцчшщьыъэюяАБВГДЕЁЗЖИЙКЛМНОПРСТУФХЦЧШЩЬЫЪЭЮЯ,.[]{}()=+-−*&^%$#@!~;:0123456789§/\\|\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ ')\n",
    "\n",
    "    alphabet.append(\"'\")\n",
    "\n",
    "    allowed = set(currencies + alphabet)\n",
    "\n",
    "    cleaned_text = [sym for sym in text if sym in allowed]\n",
    "    cleaned_text = ''.join(cleaned_text)\n",
    "\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для запуска предобработки \n",
    "\n",
    "def process(pipeline, text='Строка', keep_pos=True, keep_punct=False):\n",
    "    entities = {'PROPN'}\n",
    "    named = False\n",
    "    memory = []\n",
    "    mem_case = None\n",
    "    mem_number = None\n",
    "    tagged_propn = []\n",
    "\n",
    "    # обрабатываем текст, получаем результат в формате conllu:\n",
    "    processed = pipeline.process(text)\n",
    "\n",
    "    # пропускаем строки со служебной информацией:\n",
    "    content = [l for l in processed.split('\\n') if not l.startswith('#')]\n",
    "\n",
    "    # извлекаем из обработанного текста леммы, тэги и морфологические характеристики\n",
    "    tagged = [w.split('\\t') for w in content if w]\n",
    "\n",
    "    for t in tagged:\n",
    "        if len(t) != 10:\n",
    "            continue\n",
    "        (word_id, token, lemma, pos, xpos, feats, head, deprel, deps, misc) = t\n",
    "        token = clean_token(token, misc)\n",
    "        lemma = clean_lemma(lemma, pos)\n",
    "        if not lemma or not token:\n",
    "            continue\n",
    "        if pos in entities:\n",
    "            if '|' not in feats:\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "                continue\n",
    "            morph = {el.split('=')[0]: el.split('=')[1] for el in feats.split('|')}\n",
    "            if 'Case' not in morph or 'Number' not in morph:\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "                continue\n",
    "            if not named:\n",
    "                named = True\n",
    "                mem_case = morph['Case']\n",
    "                mem_number = morph['Number']\n",
    "            if morph['Case'] == mem_case and morph['Number'] == mem_number:\n",
    "                memory.append(lemma)\n",
    "                if 'SpacesAfter=\\\\n' in misc or 'SpacesAfter=\\s\\\\n' in misc:\n",
    "                    named = False\n",
    "                    past_lemma = '::'.join(memory)\n",
    "                    memory = []\n",
    "                    tagged_propn.append(past_lemma + '_PROPN ')\n",
    "            else:\n",
    "                named = False\n",
    "                past_lemma = '::'.join(memory)\n",
    "                memory = []\n",
    "                tagged_propn.append(past_lemma + '_PROPN ')\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "        else:\n",
    "            if not named:\n",
    "                if pos == 'NUM' and token.isdigit():  # Заменяем числа на xxxxx той же длины\n",
    "                    lemma = num_replace(token)\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "            else:\n",
    "                named = False\n",
    "                past_lemma = '::'.join(memory)\n",
    "                memory = []\n",
    "                tagged_propn.append(past_lemma + '_PROPN ')\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "\n",
    "    if not keep_punct:\n",
    "        tagged_propn = [word for word in tagged_propn if word.split('_')[1] != 'PUNCT']\n",
    "    if not keep_pos:\n",
    "        tagged_propn = [word.split('_')[0] for word in tagged_propn]\n",
    "    return tagged_propn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# собственно обработка \n",
    "\n",
    "standard_library.install_aliases()\n",
    "\n",
    "def tag_ud(text, outputfile, process_pipeline):\n",
    "    # текст нужно передать в виде строки с переносами строк \\n\n",
    "    outputlist = []\n",
    "\n",
    "    print('Processing input...', file=sys.stderr)\n",
    "    for line in text.split(\"\\n\"): \n",
    "        # line = unify_sym(line.strip()) # здесь могла бы быть ваша функция очистки текста\n",
    "        output = process(process_pipeline, text=line)\n",
    "        \n",
    "        # записываем строку в лист\n",
    "        outputlist.append(' '.join(output))\n",
    "    \n",
    "    with codecs.open(outputfile, \"w\", \"utf-8\") as output:\n",
    "        output.write(\"\\n\".join(outputlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading the model...\n",
      "Processing input...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dom\n",
      "{'r-concr_t-constr_top-contain',_'r-concr_t-org'}.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing input...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r-concr_t-constr_top-contain',_'r-concr_t-space'}.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing input...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r-concr_t-constr_top-contain'}.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing input...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r-concr_t-group_pt-set_sc-hum',_'r-concr_t-constr_top-contain',_'r-concr_t-space'}.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing input...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r-concr_t-group_pt-set_sc-hum'}.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing input...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r-concr_t-org',_'r-concr_t-constr_top-contain',_'r-concr_t-space'}.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing input...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r-concr_t-org'}.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing input...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r-concr_t-space'}.txt\n",
      "glava\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing input...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r-concr_der-shift_dt-partb'}.txt\n",
      "{'r-concr_pt-partb_pc-hum'}.txt"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing input...\n",
      "Processing input...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'r-concr_t-hum'}.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing input...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r-concr_t-text_pt-part_pc-text'}.txt\n",
      "luk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing input...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r-concr_t-plant_t-fruit_t-food_pt-aggr'}.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing input...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r-concr_t-tool-weapon_top-arc'}.txt\n",
      "organ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing input...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r-concr_der-shift_dt-partb'}.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing input...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r-concr_pt-partb_pc-hum_pc-animal_hi-class'}.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing input...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r-concr_t-org_hi-class'}.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing input...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r-concr_t-tool-mus'}.txt\n",
      "vid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing input...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r-abstr_der-shift'}.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing input...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r-abstr_r-concr_pt-set_sc-X'}.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing input...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r-abstr_t-ment'}.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing input...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r-abstr_t-perc_der-v'}.txt\n",
      "{'r-concr_t-doc'}.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing input...\n",
      "Processing input...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r-concr_t-workart'}.txt\n"
     ]
    }
   ],
   "source": [
    "# запуск (сразу на всех файлах)\n",
    "modelfile='C:\\\\Users\\\\boss\\\\Documents\\\\Diploma\\\\RNC_Subcorpus\\\\udpipe_syntagrus.model'\n",
    "print('\\nLoading the model...', file=sys.stderr)\n",
    "model = Model.load(modelfile)\n",
    "process_pipeline = Pipeline(model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')\n",
    "\n",
    "common_path = \"C:\\\\Users\\\\boss\\\\Documents\\\\Diploma\\\\RNC_Subcorpus\\\\\"\n",
    "\"\"\"\n",
    "instrumenty_input_path = \"!instrumenty\\\\raw_texts\\\\without_punctuation\\\\\"\n",
    "instrumenty_output_path = \"!instrumenty\\\\preprocessed\\\\\"\n",
    "keys1 = [\"britva\", \"karandash\", \"kosa\", \"lom\", \"lopata\", \"metla\", \"nozhnitsy\", \"schotka\", \"topor\", \"venik\", \"veslo\"]\n",
    "for key in keys1:\n",
    "    print(key)\n",
    "    instrument_input_files = os.listdir(common_path + instrumenty_input_path + key + \"\\\\\")\n",
    "    for file in instrument_input_files:\n",
    "        if file.endswith(\".txt\") and file != \"bad.txt\":\n",
    "            outputfile = common_path + instrumenty_output_path + key+ \"\\\\\" + file\n",
    "            with codecs.open(common_path + instrumenty_input_path + key + \"\\\\\" + file, 'r', encoding='utf-8') as input_f:\n",
    "                text = input_f.read()\n",
    "                tag_ud(text, outputfile, process_pipeline)\n",
    "                print(file)\n",
    "\"\"\"\n",
    "\n",
    "raznoje_input_path = \"!raznoje\\\\raw_texts\\\\without_punctuation\\\\\"\n",
    "raznoje_output_path = \"!raznoje\\\\preprocessed\\\\\"\n",
    "keys2 = [\"dom\", \"glava\", \"luk\", \"organ\", \"vid\"]\n",
    "for key in keys2:\n",
    "    print(key)\n",
    "    raznoje_input_files = os.listdir(common_path + raznoje_input_path + key + \"\\\\good\\\\\")\n",
    "    for file in raznoje_input_files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            outputfile = common_path + raznoje_output_path + key + \"\\\\\" + file\n",
    "            with codecs.open(common_path + raznoje_input_path + key + \"\\\\good\\\\\" + file, 'r', encoding='utf-8') as input_f:\n",
    "                text = input_f.read() \n",
    "                # в \"разном\" убираем подчёркивания целевого слова: так не очень хорошо лемматизируется \n",
    "                text = text.replace(\"_\", \"\")\n",
    "                tag_ud(text, outputfile, process_pipeline)\n",
    "                print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Удаляем функциональные слова  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"Вы также можете удалить стоп-слова, воспользовавшись, например, списком стоп-слов в NLTK https://pythonspot.com/nltk-stop-words/ или на основании того, что слово было распознано как функциональная часть речи (предлоги союзы частицы?) - \n",
    "именно так производилась фильтрация в новых моделях.\"\n",
    "\n",
    "Universal Dependencies Tags - https://www.sketchengine.eu/universal-pos-tags/\n",
    "# предлоги ADP\n",
    "# союзы SCONJ CCONJ\n",
    "# частицы PART\n",
    "# а ещё есть interjection INTJ, symbol SYM, other X, auxilary AUX (быть - в составных формах)\n",
    "# Ещё есть determiner DET (этот, твой, тот) - его не убираем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nfor key in keys2:\\n    files = os.listdir(path_to_raznoje + key + \"\\\\\")\\n    for file in files:\\n        with codecs.open(path_to_raznoje + key + \"\\\\\" + file, \"r\", \"utf-8\") as input_f:\\n            with codecs.open(path_to_raznoje + \"without_some_POS\\\\\" + key + \"\\\\\" + file, \"w\", \"utf-8\") as output_f: \\n                lines = input_f.read().split(\"\\n\")\\n                new_lines = []\\n                for line in lines:\\n                    if line == \"\" or line.startswith(\"#\"):  # на самом деле это условие уже не нужно\\n                        new_line = []\\n                        pass\\n                    else:\\n                        try:\\n                            line = line[line.index(\"_NUM\")+5:]  # обрезаем номера строк\\n                        except:\\n                            print(key, file)\\n                            print(repr(line))\\n                        new_line = []\\n                        for word in line.split():\\n                            if (word.endswith(\"_PART\") or word.endswith(\"_SCONJ\") or word.endswith(\"_CCONJ\") or                                 word.endswith(\"_X\") or word.endswith(\"_ADP\") or word.endswith(\"_INTJ\") or                                 word.endswith(\"_SYM\") or word.endswith(\"_AUX\")):\\n                                pass\\n                            else:\\n                                new_line.append(word)\\n                    new_lines.append(\" \".join(new_line))\\n                output_f.write(\"\\n\".join(new_lines))  \\n'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# из файлов убираем ADP SCONJ CCONJ PART INTJ SYM X AUX, \n",
    "# а также в разном - первые строки, начинающиеся с # (где есть) и номера строк в начале файлов\n",
    "path_to_instrumenty = \"C:\\\\Users\\\\boss\\\\Documents\\\\Diploma\\\\RNC_Subcorpus\\\\!instrumenty\\\\preprocessed\\\\\"\n",
    "path_to_raznoje = \"C:\\\\Users\\\\boss\\\\Documents\\\\Diploma\\\\RNC_Subcorpus\\\\!raznoje\\\\preprocessed\\\\\"\n",
    "\n",
    "keys1 = [\"britva\", \"karandash\", \"kosa\", \"lom\", \"lopata\", \"metla\", \"nozhnitsy\", \"schotka\", \"topor\", \"venik\", \"veslo\"]\n",
    "keys2 = [\"dom\", \"glava\", \"luk\", \"organ\", \"vid\"]\n",
    "\n",
    "\n",
    "for key in keys1:\n",
    "    files = os.listdir(path_to_instrumenty + key + \"\\\\\")\n",
    "    for file in files:\n",
    "        with codecs.open(path_to_instrumenty + key + \"\\\\\" + file, \"r\", \"utf-8\") as input_f:\n",
    "            with codecs.open(path_to_instrumenty + \"without_some_POS\\\\\" + key + \"\\\\\" + file, \"w\", \"utf-8\") as output_f: \n",
    "                lines = input_f.read().split(\"\\n\")\n",
    "                new_lines = []\n",
    "                for line in lines:\n",
    "                    new_line = []\n",
    "                    for word in line.split():\n",
    "                        if (word.endswith(\"_PART\") or word.endswith(\"_SCONJ\") or word.endswith(\"_CCONJ\") or \\\n",
    "                            word.endswith(\"_X\") or word.endswith(\"_ADP\") or word.endswith(\"_INTJ\") or \\\n",
    "                            word.endswith(\"_SYM\") or word.endswith(\"_AUX\")):\n",
    "                            pass\n",
    "                        else:\n",
    "                            new_line.append(word)\n",
    "                    new_lines.append(\" \".join(new_line))\n",
    "                output_f.write(\"\\n\".join(new_lines))        \n",
    "\"\"\"\n",
    "for key in keys2:\n",
    "    files = os.listdir(path_to_raznoje + key + \"\\\\\")\n",
    "    for file in files:\n",
    "        with codecs.open(path_to_raznoje + key + \"\\\\\" + file, \"r\", \"utf-8\") as input_f:\n",
    "            with codecs.open(path_to_raznoje + \"without_some_POS\\\\\" + key + \"\\\\\" + file, \"w\", \"utf-8\") as output_f: \n",
    "                lines = input_f.read().split(\"\\n\")\n",
    "                new_lines = []\n",
    "                for line in lines:\n",
    "                    if line == \"\" or line.startswith(\"#\"):  # на самом деле это условие уже не нужно\n",
    "                        new_line = []\n",
    "                        pass\n",
    "                    else:\n",
    "                        try:\n",
    "                            line = line[line.index(\"_NUM\")+5:]  # обрезаем номера строк\n",
    "                        except:\n",
    "                            print(key, file)\n",
    "                            print(repr(line))\n",
    "                        new_line = []\n",
    "                        for word in line.split():\n",
    "                            if (word.endswith(\"_PART\") or word.endswith(\"_SCONJ\") or word.endswith(\"_CCONJ\") or \\\n",
    "                                word.endswith(\"_X\") or word.endswith(\"_ADP\") or word.endswith(\"_INTJ\") or \\\n",
    "                                word.endswith(\"_SYM\") or word.endswith(\"_AUX\")):\n",
    "                                pass\n",
    "                            else:\n",
    "                                new_line.append(word)\n",
    "                    new_lines.append(\" \".join(new_line))\n",
    "                output_f.write(\"\\n\".join(new_lines))  \n",
    "\"\"\"                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверяем, что в каждой строке есть целевое слово - теперь да!\n",
    "# для разного\n",
    "translit_raznoje = {\"dom\":\"дом\",\n",
    "\"glava\":\"глава\",\n",
    "\"luk\":\"лук\",\n",
    "\"organ\":\"орган\",\n",
    "\"vid\":\"вид\"}\n",
    "\n",
    "path_to_raznoje = \"C:\\\\Users\\\\boss\\\\Documents\\\\Diploma\\\\RNC_Subcorpus\\\\!raznoje\\\\preprocessed\\\\without_some_POS\\\\\" \n",
    "\n",
    "for element in keys2:\n",
    "    if element.startswith(\"NEW\"):\n",
    "        print(\"______________________\" + element + \"______________________\")\n",
    "        files = os.listdir(path_to_raznoje + element + \"\\\\\")\n",
    "        word_we_look_for = translit_raznoje[element]\n",
    "        for file in files:\n",
    "            print(file)\n",
    "            j = 0\n",
    "            with codecs.open(path_to_raznoje + element + \"\\\\\" + file, \"r\", \"utf-8\") as file_to_prove:\n",
    "                lines = file_to_prove.read().split(\"\\n\")\n",
    "                for i, line in enumerate(lines):\n",
    "                    if j < 10:\n",
    "                        if ((word_we_look_for+\"_NOUN\") not in line.split()) and (word_we_look_for+\"_PROPN\" not in line.split()):\n",
    "                            print(i, line)\n",
    "                            j += 1\n",
    "            print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Исправление ошибок "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Во всех файлах есть целевые слова, иногда много, которые разобрались неправильно. Нужно что-то с этим делать.\n",
    "Непосредственно на процесс лемматизации повлиять не можем, потому что он осуществляется моделью в пайплайне...\n",
    "Для разного у нас был словарь, в котором для каждого слова мы знаем его позицию в строке. Так как здесь на выходе слов столько же, сколько в raw файлах, можем находить эту позицию и заменять то, что на нём, на правильную разметку \"слово_NOUN\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Для разного: сначала составляем словарь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\boss\\\\Documents\\\\Diploma\\\\RNC_Subcorpus\\\\!raznoje\\\\'\n",
    "os.chdir(path)\n",
    "files = os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dom.txt\n",
      "glava.txt\n",
      "luk.txt\n",
      "organ.txt\n",
      "vid.txt\n"
     ]
    }
   ],
   "source": [
    "dict_with_files = dict()\n",
    "for file in files:\n",
    "    if file.endswith(\".txt\") and file != \"DICT.txt\":\n",
    "        print(file)\n",
    "        with codecs.open(file, \"r\", \"utf-8\") as opened:\n",
    "            file = file[:-4]\n",
    "            dict_with_files[file] = []\n",
    "            lines = opened.readlines()\n",
    "            for i, line in enumerate(lines):\n",
    "                list_for_line = []\n",
    "                annotation_index = 0\n",
    "                while True:\n",
    "                    try:\n",
    "                        annotation_index = line.index(\"ana lex\", annotation_index+6)  # начало аннотации в целом\n",
    "                        gr_index = line.index(\"gr=\", annotation_index)\n",
    "                        word_index = line.index(\"/>\", annotation_index)\n",
    "                        try:\n",
    "                            SEMF_index = line.index(\"SEMF=\", annotation_index)\n",
    "                        except ValueError:\n",
    "                            try: \n",
    "                                SEMF_index = line.index(\"SEMF2=\", annotation_index)\n",
    "                            except ValueError:\n",
    "                                SEMF_index = 0\n",
    "                        try:\n",
    "                            sem_index = line.index(\"sem=\", annotation_index)\n",
    "                        except:\n",
    "                            sem_index = 0\n",
    "                    except:\n",
    "                        break\n",
    "\n",
    "                    lexema = line[annotation_index+9:line.index(\"'\",annotation_index+9)].strip()\n",
    "                    grammar = line[gr_index+4:line.index(\"'\",gr_index+4)].strip()\n",
    "                    try:\n",
    "                        word_end = line.index(\"<\", word_index)\n",
    "                    except ValueError:\n",
    "                        word_end = len(line)-1\n",
    "                        print(i, line)\n",
    "                        \n",
    "                    word = line[word_index+2:word_end].replace(\"`\", \"\").strip()\n",
    "\n",
    "                    if (word_index > sem_index) and (sem_index != 0):\n",
    "                        sem = line[sem_index+5:line.index(\"/>\", sem_index)-1].strip()\n",
    "                    elif (word_index > SEMF_index) and (SEMF_index != 0):\n",
    "                        sem = line[SEMF_index+6:line.index(\"/>\", SEMF_index)-1].strip()\n",
    "                    else:\n",
    "                        sem = \"\"\n",
    "                    \n",
    "                    list_for_line.append([lexema, grammar, sem, word])\n",
    "                dict_with_files[file].append(list_for_line)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаём класс для неоднозначных слов\n",
    "class Word_to_Disambiguate(object):\n",
    "    \n",
    "    def __init__(self, lemmas, sem_set, string, num_of_string, num_in_string):\n",
    "        self.lemmas = lemmas  # set с леммами (set - потому что их мб несколько, из разных аннотаций)\n",
    "        self.sem_set = sem_set  # set с семантикой\n",
    "        self.string = string  # строка из raw файла\n",
    "        self.num_of_string = num_of_string  # номер строки в файле (в raw и в original равны)\n",
    "        self.num_in_string = num_in_string  # номер слова в строке, в raw и original равны, если считать те, где word[3] != ''\n",
    "    \n",
    "    def __repr__(self):\n",
    "        string_to_return = \"lemmas: %s \\n sems: %s \\n num_in_string: %s \\n %s - %s\" % (str(self.lemmas), \n",
    "                                                                                       str(self.sem_set),\n",
    "                                                                                      str(self.num_in_string), \n",
    "                                                                                      str(self.num_of_string), \n",
    "                                                                                          self.string)\n",
    "        return(string_to_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ДЛЯ РАЗНЫХ\n",
    "translit = {\"dom\":\"Дом\",\n",
    "\"glava\":\"Глава\",\n",
    "\"luk\":\"Лук\",\n",
    "\"organ\":\"Орган\",\n",
    "\"vid\":\"Вид\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dom\n",
      "glava\n",
      "luk\n",
      "organ\n",
      "vid\n"
     ]
    }
   ],
   "source": [
    "# словарь, в котором будут все целевые слова\n",
    "dict_with_words_to_disambiguate = dict()\n",
    "\n",
    "for name in dict_with_files:\n",
    "    dict_with_words_to_disambiguate[name] = []\n",
    "    with codecs.open(\"C:\\\\Users\\\\boss\\\\Documents\\\\Diploma\\\\RNC_Subcorpus\\\\!raznoje\\\\raw_texts\\\\without_punctuation\\\\\" + \n",
    "                    name + \".txt\", \"r\", \"utf-8\") as raw_text:\n",
    "        raw_lines = raw_text.readlines()\n",
    "        print(name)\n",
    "        for j, line in enumerate(dict_with_files[name]):\n",
    "            sem_set_for_one_word = set()\n",
    "            lemmas = set()\n",
    "            num_of_string = j\n",
    "            string = raw_lines[j]\n",
    "            num_in_string = -1  # сначала будет равен 0 (тк +1), а потом будем искать следующее вхождение слова, начиная с него\n",
    "            i = 0\n",
    "            while i<len(line):  # проходимся по словам\n",
    "                # сначала разбираемся с инстансами, у которых было несколько аннотаций\n",
    "                if line[i][3] == \"\" and (line[i][0] == translit[name] or line[i][0] == translit[name].lower()):\n",
    "                    lemmas.add(line[i][0])\n",
    "                    for sem in line[i][2].split(\"|\"):\n",
    "                        sem_set_for_one_word.add(sem.replace(\"'\",\"\").replace(\"SEMF=\",\"\").strip())  \n",
    "                    for counter in range(1, 10):\n",
    "                        if line[i+counter][3] == \"\":\n",
    "                            lemmas.add(line[i+counter][0])\n",
    "                            for sem in line[i][2].split(\"|\"):\n",
    "                                sem_set_for_one_word.add(sem.replace(\"'\",\"\").replace(\"SEMF=\",\"\").strip())\n",
    "                        else:\n",
    "                            lemmas.add(line[i+counter][0])\n",
    "                            for sem in line[i][2].split(\"|\"):\n",
    "                                sem_set_for_one_word.add(sem.replace(\"'\",\"\").replace(\"SEMF=\",\"\").strip())\n",
    "                            # здесь предусматриваем случай, когда в строке мб несколько одинаковых словоформ: поэтому \n",
    "                            # в каждой новой строке начинаем искать с нуля, а потом - с предыдущего вхождения\n",
    "                            try:\n",
    "                                num_in_string = string.split().index(line[i+counter][3], num_in_string+1)\n",
    "                            except ValueError:  # не нашли в строке слово\n",
    "                                print(\"Multiple annotations\")\n",
    "                                print(num_of_string, line[i+counter][3], string)\n",
    "                            \n",
    "                            # закончили с этим словом\n",
    "                            dict_with_words_to_disambiguate[name].append(Word_to_Disambiguate(lemmas, sem_set_for_one_word, \n",
    "                                                                                              string, num_of_string, \n",
    "                                                                                              num_in_string))\n",
    "                            sem_set_for_one_word = set()\n",
    "                            lemmas = set()\n",
    "                            i += counter + 1\n",
    "                            break\n",
    "                            \n",
    "                elif (line[i][0] == translit[name] or line[i][0] == translit[name].lower()): # если у слова одна аннотация\n",
    "                    lemmas = set([line[i][0]])\n",
    "                    for sem in line[i][2].split(\"|\"):\n",
    "                        sem_set_for_one_word.add(sem.replace(\"'\",\"\").replace(\"SEMF=\",\"\").replace(\"SEMF2=\",\"\").strip())\n",
    "                    try:\n",
    "                        num_in_string = string.split().index(line[i][3], num_in_string+1)\n",
    "                    except ValueError:  # не нашли в строке слово\n",
    "                        print(\"One annotation\")\n",
    "                        print(str(num_of_string) + \" /\"+line[i][3]+\"/ \" + string)\n",
    "                    dict_with_words_to_disambiguate[name].append(Word_to_Disambiguate(lemmas, sem_set_for_one_word, string,\n",
    "                                                                                     num_of_string, num_in_string))\n",
    "                    # закончили с этим словом\n",
    "                    sem_set_for_one_word = set()\n",
    "                    lemmas = set()\n",
    "                    i += 1\n",
    "                    \n",
    "                else:  # это не целевое слово\n",
    "                    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаём новый словарь, чтобы там были только семы без примеров\n",
    "words_to_disambiguate_wo_exs = dict()\n",
    "for name in dict_with_words_to_disambiguate:\n",
    "    words_to_disambiguate_wo_exs[name] = []\n",
    "    for instance in dict_with_words_to_disambiguate[name]:\n",
    "        new_sem_set = set()\n",
    "        for element in instance.sem_set:\n",
    "            try:\n",
    "                ex_ind = element.index(\"Ex\")\n",
    "            except ValueError:\n",
    "                ex_ind = len(element)\n",
    "            new_sem = element[:ex_ind].strip()\n",
    "            new_sem_set.add(new_sem)\n",
    "        new_instance = Word_to_Disambiguate(instance.lemmas, new_sem_set, instance.string, \n",
    "                                            instance.num_of_string, instance.num_in_string)\n",
    "        words_to_disambiguate_wo_exs[name].append(new_instance)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmas: {'дом'} \n",
      " sems: {'r:concr t:constr top:contain'} \n",
      " num_in_string: 2 \n",
      " 0 - Добротный деревянный дом рядом с крыльцом берёза а от дома дорога ведёт к Богоявленской церкви\n",
      "\n",
      "lemmas: {'дом'} \n",
      " sems: {'r:concr t:constr top:contain'} \n",
      " num_in_string: 9 \n",
      " 0 - Добротный деревянный дом рядом с крыльцом берёза а от дома дорога ведёт к Богоявленской церкви\n",
      "\n",
      "lemmas: {'дом'} \n",
      " sems: {'r:concr t:constr top:contain'} \n",
      " num_in_string: 3 \n",
      " 1 - Это и есть дом где Андрей Тарковский провёл довоенное и военное детство\n",
      "\n",
      "lemmas: {'дом'} \n",
      " sems: {'r:concr t:space', 'r:concr t:constr top:contain'} \n",
      " num_in_string: 2 \n",
      " 2 - Входим в дом открыв дверь попадаем в кухню печь налево окно рядом небольшой стол керосиновая лампа и три деревянных стула\n",
      "\n",
      "lemmas: {'дом'} \n",
      " sems: {'r:concr t:org', 'r:concr t:constr top:contain'} \n",
      " num_in_string: 1 \n",
      " 3 - Посещение дома музея лишь часть обширной программы Дней Тарковского в Юрьевце\n",
      "\n",
      "lemmas: {'глава'} \n",
      " sems: {'r:concr pt:partb pc:hum'} \n",
      " num_in_string: 53 \n",
      " 0 - Крутизна горы суть препятствия кои Петр имел производя в действо свои намерения змея в пути лежащая коварство и злоба искавшие кончины его за введение новых нравов древняя одежда звериная кожа и весь простой убор коня и всадника суть простыя и грубыя нравы и непросвещение кои Петр нашёл в народе которой он преобразовать вознамерился глава лаврами венчанная победитель бо был прежде нежели законодатель вид мужественной и мощной и крепость преобразователя простёртая рука покровительствующая как её называет Дидеро и взор весёлый суть внутренное уверение достигшия цели и рука простёртая являет 11 что крепкия муж преодолев все стремлению его противявшияся пороки покров свой даёт всем чадами его называющимся\n",
      "\n",
      "lemmas: {'глава'} \n",
      " sems: {'r:concr pt:partb pc:hum'} \n",
      " num_in_string: 10 \n",
      " 1 - Всадник без стремян в полукафтанье кушаком препоясан облечённой багряницею имеющ главу лаврами венчанную и десницу простёртую\n",
      "\n",
      "lemmas: {'глава'} \n",
      " sems: {'r:concr pt:partb pc:hum'} \n",
      " num_in_string: 3 \n",
      " 2 - Была полна моя глава\n",
      "\n",
      "lemmas: {'глава'} \n",
      " sems: {'r:concr pt:partb pc:hum'} \n",
      " num_in_string: 13 \n",
      " 3 - Благословенно да будет явление твоё речет преемница Престола его и дел и преклоняет главу\n",
      "\n",
      "lemmas: {'глава'} \n",
      " sems: {'r:concr pt:partb pc:hum'} \n",
      " num_in_string: 10 \n",
      " 4 - Органы раз тебе простили два простили а на третий раз главу прочь\n",
      "\n",
      "lemmas: {'лука', 'лук'} \n",
      " sems: {'r:concr t:tool:weapon top:arc', 'r:concr t:food pt:aggr sc:part(plant)'} \n",
      " num_in_string: 1 \n",
      " 0 - Скажи лук\n",
      "\n",
      "lemmas: {'лук'} \n",
      " sems: {'r:concr t:tool:weapon top:arc'} \n",
      " num_in_string: 6 \n",
      " 1 - Храбрый Джон отправляется на войну с луком но без стрел\n",
      "\n",
      "lemmas: {'лука', 'лук'} \n",
      " sems: {'r:concr t:plant t:fruit t:food pt:aggr'} \n",
      " num_in_string: 19 \n",
      " 2 - Выловили их дикари и говорят каждому чтобы получить пирогу и умотать отсюда надо заплатить сто долларов или съесть ведро лука или тебя все племя трахает\n",
      "\n",
      "lemmas: {'лука', 'лук'} \n",
      " sems: {'r:concr t:plant t:fruit t:food pt:aggr'} \n",
      " num_in_string: 1 \n",
      " 3 - Ведро лука мне давясь доедает и уплывает\n",
      "\n",
      "lemmas: {'лука', 'лук'} \n",
      " sems: {'r:concr t:plant t:fruit t:food pt:aggr'} \n",
      " num_in_string: 1 \n",
      " 4 - Ведро лука мне\n",
      "\n",
      "lemmas: {'орган'} \n",
      " sems: {'r:concr t:org hi:class'} \n",
      " num_in_string: 13 \n",
      " 0 - Его сиятельство после признания положения катастрофическим изволило озлобиться и спустило с поводка правоохранительные органы начальники которых тоже надо думать жили в каком-то другом измерении и не видели творящегося вокруг них мусорного апокалипсиса\n",
      "\n",
      "lemmas: {'орган'} \n",
      " sems: {'r:concr t:org hi:class'} \n",
      " num_in_string: 7 \n",
      " 1 - Здесь как лапидарно сказано в сообщении правоохранительными органами предотвращена попытка незаконного освобождения из колонии строгого строгого заметьте режима четырех особо опасных заключённых\n",
      "\n",
      "lemmas: {'орган'} \n",
      " sems: {'r:concr t:org hi:class'} \n",
      " num_in_string: 7 \n",
      " 2 - Могут возникнуть проблемы не только с правоохранительными органами но и со своими коллегами\n",
      "\n",
      "lemmas: {'орган'} \n",
      " sems: {'r:concr t:org hi:class'} \n",
      " num_in_string: 20 \n",
      " 3 - Впрочем уголовное дело никаких перспектив не имеет стоит только следователям или судьям слово сказать против Чубайса как во всех правоохранительных органах края погаснет свет а поближе к зиме перестанет работать отопление\n",
      "\n",
      "lemmas: {'орган'} \n",
      " sems: {'r:concr t:org hi:class'} \n",
      " num_in_string: 6 \n",
      " 4 - Тридцать с лишним лет службы в органах три развода потерянная вера одиночество на закате жизни грошовая зарплата и отсутствие перспектив\n",
      "\n",
      "lemmas: {'вид'} \n",
      " sems: {'r:abstr t:perc der:v'} \n",
      " num_in_string: 2 \n",
      " 0 - По внешнему виду пояса оказались идентичными тем что сработали в Москве во время тушинского праздника\n",
      "\n",
      "lemmas: {'вид'} \n",
      " sems: {'r:abstr t:perc der:v'} \n",
      " num_in_string: 10 \n",
      " 1 - Террорист спускаясь с гор одеваясь в форму российской милиции делает вид что борется с терроризмом\n",
      "\n",
      "lemmas: {'вид'} \n",
      " sems: {'r:abstr t:perc der:v'} \n",
      " num_in_string: 4 \n",
      " 2 - В Кремле продолжают делать вид что никакой войны нет\n",
      "\n",
      "lemmas: {'вид'} \n",
      " sems: {'r:abstr t:perc der:v'} \n",
      " num_in_string: 3 \n",
      " 3 - Президент Путин делает вид что не в курсе\n",
      "\n",
      "lemmas: {'вид'} \n",
      " sems: {'r:abstr t:perc der:v'} \n",
      " num_in_string: 21 \n",
      " 4 - Взять хотя бы дом N8на Дворцовой набережной в котором мы с вами беседуем из его окон открывается один из самых лучших видов на Адмиралтейство\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name in words_to_disambiguate_wo_exs:\n",
    "    for i, instance in enumerate(words_to_disambiguate_wo_exs[name]):\n",
    "        if i < 5:\n",
    "            print(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для преобразования вложенных списков в невложенные\n",
    "from collections import Iterable\n",
    "\n",
    "def flatten_list(items):\n",
    "    \"\"\"Yield items from any nested iterable; see Reference.\"\"\"\n",
    "    for x in items:\n",
    "        if isinstance(x, Iterable) and not isinstance(x, (str, bytes)):\n",
    "            for sub_x in flatten(x):\n",
    "                yield sub_x\n",
    "        else:\n",
    "            yield x"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Открываем в preprocessed + dom (например) все файлы последовательно, смотрим в словаре по ключу dom - по ключу \"название файла\" с заменёнными \"_\" и \":\". Там для каждого инстанса по номеру в строке, которую мы разделили по пробелам и по \"::\" - заменяем на translit[name].lower() + \"_NOUN\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "x_NUM посещение_NOUN дома_NOUN музей_NOUN лишь_PART часть_NOUN обширный_ADJ программа_NOUN день_PROPN  тарковский_PROPN в_ADP юрьевец_PROPN \n"
     ]
    }
   ],
   "source": [
    "with codecs.open(path + name + \"\\\\\" + file_with_name_as_set, \"r\", \"utf-8\") as input_f:\n",
    "    input_lines = input_f.read().split(\"\\n\")\n",
    "    print(type(input_lines))\n",
    "    print(input_lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glava {'r-concr_t-hum'}.txt\n",
      "759 Вместо администрации одного сельского округа будет 10 15глав администраций во входящих в него деревнях насколько же будут раздуты штаты управленческого аппарата При новом дроблении органов МСУ война между муниципалитетами вспыхнет с новой силой\n",
      "10-15глав\n"
     ]
    }
   ],
   "source": [
    "path = \"C:\\\\Users\\\\boss\\\\Documents\\\\Diploma\\\\RNC_Subcorpus\\\\!raznoje\\\\preprocessed\\\\\"\n",
    "\n",
    "for i, name in enumerate(words_to_disambiguate_wo_exs):  # dom \n",
    "    sem_sets_as_file_names = os.listdir(path + name + \"\\\\\")\n",
    "    for file_with_name_as_set in sem_sets_as_file_names:\n",
    "        set_as_string = file_with_name_as_set[:-4].replace(\"_\", \" \").replace(\"-\",\":\")\n",
    "        set_as_set = eval(set_as_string)  # convert a string representation into an actual set\n",
    "        \n",
    "        # создаём old_raw_files, если мы вносили изменения и, следовательно, есть старый файл (начинающийся на _)\n",
    "        old_one = \"C:\\\\Users\\\\boss\\\\Documents\\\\Diploma\\\\RNC_Subcorpus\\\\!raznoje\\\\raw_texts\\\\without_punctuation\\\\\" + \\\n",
    "        name + \"\\\\good\\\\_\" + file_with_name_as_set\n",
    "        there_is_old_one = False\n",
    "        if os.path.isfile(old_one):\n",
    "            there_is_old_one = True\n",
    "            with codecs.open(old_one, \"r\", \"utf-8\") as old_raw_file:\n",
    "                old_raw_lines = old_raw_file.read().split(\"\\n\")\n",
    "                for q, line in enumerate(old_raw_lines):\n",
    "                    old_raw_lines[q] = line.replace(\"_\", \"\")\n",
    "        \n",
    "        \n",
    "        # open raw file - to find index of strings where to replace target words\n",
    "        with codecs.open(\"C:\\\\Users\\\\boss\\\\Documents\\\\Diploma\\\\RNC_Subcorpus\\\\!raznoje\\\\raw_texts\\\\without_punctuation\\\\\" + \n",
    "                        name + \"\\\\good\\\\\" + file_with_name_as_set, \"r\", \"utf-8\") as raw_file:\n",
    "            raw_lines = raw_file.read().split(\"\\n\")\n",
    "            for l, line in enumerate(raw_lines):\n",
    "                # убираем подчёркивания слов, чтобы мы потом могли найти строку \"instance.num_of_string + instance.string\" в\n",
    "                # этом списке и запомнить её индекс\n",
    "                raw_lines[l] = line.replace(\"_\", \"\")\n",
    "            \n",
    "        with codecs.open(path + name + \"\\\\\" + file_with_name_as_set, \"r\", \"utf-8\") as pos_input_f:\n",
    "            input_lines = pos_input_f.read().split(\"\\n\")\n",
    "\n",
    "        for j, line in enumerate(input_lines):\n",
    "            input_lines[j] = line.split()  # теперь элементы списка - списки со словами в соответствующей строке\n",
    "        # но ещё нужно разделить по ::, потому что так склеиваются имена собственные\n",
    "        # тогда, правда, мы потеряем информацию о именах собственных, поэтому нужно будет это отдельно потом обработать\n",
    "        for k, line in enumerate(input_lines):\n",
    "            for l, word in enumerate(input_lines[k]):\n",
    "                input_lines[k][l] = word.split(\"::\")\n",
    "            # приводим разбиения к одному уровню, без вложенных списков, чтобы индексы соответствовали\n",
    "            input_lines[k] = list(flatten_list(input_lines[k])) \n",
    "\n",
    "        for s, line in enumerate(input_lines):\n",
    "            if len(input_lines[s]) != len(raw_lines[s].split()):\n",
    "                print(name)\n",
    "                print(file_with_name_as_set)\n",
    "                print(raw_lines[s].split()[:10])\n",
    "                print(input_lines[s][:10])\n",
    "                print()\n",
    "                raise ValueError(\"lengths are not the same\")\n",
    "\n",
    "        with codecs.open(path + name + \"\\\\NEW_\" + file_with_name_as_set, \"w\", \"utf-8\") as output_f:\n",
    "            for p, instance in enumerate(words_to_disambiguate_wo_exs[name]):\n",
    "                if (instance.sem_set == set_as_set):\n",
    "                    try:\n",
    "                        num_of_string = raw_lines.index(str(instance.num_of_string) + \" \" + instance.string[:-1])\n",
    "                    except ValueError:\n",
    "                        try:\n",
    "                            num_of_string = raw_lines.index(\"-\" + str(instance.num_of_string) + \n",
    "                                                            \" \" + instance.string[:-1])\n",
    "                        except ValueError: \n",
    "                            # Строки instance.string нет raw_lines, потому что мы в неё вставляли дополнительные пробелы и\n",
    "                            # там она уже изменённая. Чтобы получить индекс instance.string в raw lines, мы\n",
    "                            # должны найти её индекс в ещё не изменённом файле: old_raw_lines\n",
    "                                try:\n",
    "                                    num_of_string = old_raw_lines.index(str(instance.num_of_string) + \n",
    "                                                                        \" \" + instance.string[:-1])\n",
    "                                except ValueError:\n",
    "                                    try:\n",
    "                                        num_of_string = old_raw_lines.index(\"-\" + str(instance.num_of_string) + \n",
    "                                                                            \" \" + instance.string[:-1])\n",
    "                                    except:\n",
    "                                        print(name, file_with_name_as_set)\n",
    "                                        print(\"! В OLD_RAW_LINES ошибка\")\n",
    "                                        raise ValueError(str(instance.num_of_string) + \" \" + instance.string[:-1])\n",
    "\n",
    "                    try:\n",
    "                        if there_is_old_one:  # если мы в файл добавляли пробелы\n",
    "                            # по старому файлу, по индексу слова находим форму, в которой оно стоит (сейчас индекс мб больше) \n",
    "                            word_form_we_are_searching = old_raw_lines[num_of_string].split()[instance.num_in_string+1]\n",
    "                            # теперь ищем эту форму (её индекс) в новом файле в той же строке, начиная с \n",
    "                            # её индекса в первоначальной строке (а не сначала)\n",
    "                            try:\n",
    "                                its_index = raw_lines[num_of_string].split().index(word_form_we_are_searching, \n",
    "                                                                                   instance.num_in_string + 1)\n",
    "                            except ValueError: # not in the list\n",
    "                                print(name, file_with_name_as_set)\n",
    "                                print(raw_lines[num_of_string])\n",
    "                                print(word_form_we_are_searching)\n",
    "                                \n",
    "                            # когда нашли, теперь можем заменять в input lines, потому что там индекс такой же\n",
    "                            input_lines[num_of_string][its_index] = translit[name].lower() + \"_NOUN\"\n",
    "                        else:\n",
    "                            input_lines[num_of_string][instance.num_in_string+1] = translit[name].lower() + \"_NOUN\"\n",
    "                    except IndexError:\n",
    "                        print(name)\n",
    "                        print(instance)\n",
    "                        print(num_of_string)\n",
    "                        print(input_lines[num_of_string])\n",
    "                        raise IndexError(\"Smth is wrong here\")\n",
    "\n",
    "            for m, line in enumerate(input_lines):\n",
    "                for n, word in enumerate(line):\n",
    "                    if \"_\" not in word:\n",
    "                        line[n] = word + \"_PROPN\"  # так как разделили имена собственные, у некоторых может не быть тега\n",
    "                input_lines[m] = \" \".join(line)\n",
    "            output_f.write(\"\\n\".join(input_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Для инструментов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "translit_instruments = {\"britva\":\"бритва\",\n",
    "\"karandash\":\"карандаш\", \n",
    "\"kosa\":\"коса\", \n",
    "\"lom\":\"лом\", \n",
    "\"lopata\":\"лопата\", \n",
    "\"metla\":\"метла\", \n",
    "\"nozhnitsy\":\"ножницы\", \n",
    "\"schotka\":\"щетка\", \n",
    "\"topor\":\"топор\", \n",
    "\"venik\":\"веник\", \n",
    "\"veslo\":\"весло\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_instrumenty = \"C:\\\\Users\\\\boss\\\\Documents\\\\Diploma\\\\RNC_Subcorpus\\\\!instrumenty\\\\preprocessed\\\\\"\n",
    "\n",
    "keys1 = [\"britva\", \"karandash\", \"kosa\", \"lom\", \"lopata\", \"metla\", \"nozhnitsy\", \"schotka\", \"topor\", \"venik\", \"veslo\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сначала просто смотрели на слова в предобработанных текстах, и если их лемма = транслиту файла, писали им тег _NOUN\n",
    "# потом посмотрели на ошибки и пытаемся их вручную исправить\n",
    "\n",
    "wrong_lemmas = {\"britva\":[\"бритво\", \"бритвый\"],\n",
    "\"karandash\":[\"карандаша\", \"карандаать\"], \n",
    "\"kosa\":[\"косой\", \"кос\"], \n",
    "\"lom\":[\"ломе\", \"лый\"], \n",
    "\"lopata\":[\"лопат\"], \n",
    "\"metla\":[\"метл\", \"метел\", \"метлый\", \"мётл\", \"метлоя\"], \n",
    "\"nozhnitsy\":[\"ножница\", \"ножницам\"], \n",
    "\"schotka\":[\"щеткий\", \"щетку\", \"щеток\", \"щеткой\"], \n",
    "\"topor\":[\"----------\"], \n",
    "\"venik\":[\"веника\"], \n",
    "\"veslo\":[\"весл\", \"весел\"]}\n",
    "\n",
    "for key in keys1:\n",
    "    files = os.listdir(path_to_instrumenty + key + \"\\\\\")\n",
    "    for file in files:\n",
    "        if file.startswith(\"NEW\"):\n",
    "            pass\n",
    "        else:\n",
    "            with codecs.open(path_to_instrumenty + key + \"\\\\\" + file, \"r\", \"utf-8\") as input_f:\n",
    "                lines = input_f.read().split(\"\\n\")\n",
    "\n",
    "            for l, line in enumerate(lines):\n",
    "                splitted_line = line.split()\n",
    "                \n",
    "                for w, word in enumerate(splitted_line):\n",
    "                    # сначала разделяем имена собственные, добавляя ко всем кроме последнего кусочка \"_PROPN\"\n",
    "                    splitted_PROPN = word.split(\"::\")\n",
    "                    for n, propn in enumerate(splitted_PROPN):\n",
    "                        if n != len(splitted_PROPN) - 1:\n",
    "                            splitted_PROPN[n] = splitted_PROPN[n] + \"_PROPN\"\n",
    "                    splitted_line[w] = splitted_PROPN\n",
    "                    splitted_line = list(flatten_list(splitted_line)) # приводим вложенные листы на верхний уровень\n",
    "                \n",
    "                for w, word in enumerate(splitted_line):    \n",
    "                    # теперь переходим к исправлению неправильных лемм и тегов\n",
    "                    splitted_word = word.split(\"_\")\n",
    "                    if (splitted_word[0] == translit_instruments[key]) or (splitted_word[0] in wrong_lemmas[key]):\n",
    "                        splitted_line[w] = translit_instruments[key] + \"_NOUN\"\n",
    "                lines[l] = \" \".join(splitted_line)\n",
    "\n",
    "            with codecs.open(path_to_instrumenty + key + \"\\\\NEWNEW_\" + file, \"w\", \"utf-8\") as output_f:\n",
    "                output_f.write(\"\\n\".join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "britva\n",
      "NEWNEW_r-concr_t-tool-instr_der-v.txt\n",
      "569 \n",
      "\n",
      "\n",
      "karandash\n",
      "NEWNEW_r-concr_t-tool-instr_top-rod.txt\n",
      "1852 \n",
      "\n",
      "\n",
      "kosa\n",
      "NEWNEW_r-concr_pt-additb_pc-hum_()_r-concr_t-tool-instr_()_r-concr_t-space.txt\n",
      "1038 \n",
      "\n",
      "\n",
      "lom\n",
      "NEWNEW_r-concr_t-tool-instr_top-rod_der-v_()_r-concr_pt-aggr_sc-thing_der-v.txt\n",
      "583 я_PRON взвешивать_VERB положение_NOUN подумать_VERB прикидывать_VERB и_CCONJ приказывать_VERB лий_PROPN в_ADP срочный_ADJ порядок_NOUN овладевать_VERB английский_ADJ разговорный_ADJ речь_NOUN\n",
      "614 \n",
      "\n",
      "\n",
      "lopata\n",
      "NEWNEW_r-concr_t-tool-instr.txt\n",
      "104 xx_NUM сентябрь_NOUN от_ADP калининградский_ADJ областной_ADJ дума_NOUN витаутас_PROPN представлять_VERB в_ADP первый_ADJ чтение_NOUN законопроект_NOUN о_ADP внесение_NOUN дополнение_NOUN и_CCONJ изменение_NOUN в_ADP федеральный_ADJ закон_NOUN о_ADP оружие_NOUN по_ADP вопрос_NOUN расширение_NOUN перечень_NOUN вид_NOUN гражданский_ADJ оружие_NOUN самооборона_NOUN самооборона_NOUN\n",
      "111 витаутас_PROPN депутат_NOUN калининградский_ADJ областной_ADJ дума_NOUN дума_NOUN\n",
      "1219 \n",
      "\n",
      "\n",
      "metla\n",
      "NEWNEW_r-concr_t-tool-instr_der-v.txt\n",
      "471 \n",
      "\n",
      "\n",
      "nozhnitsy\n",
      "NEWNEW_r-concr_t-tool-instr_der-s.txt\n",
      "689 \n",
      "\n",
      "\n",
      "schotka\n",
      "NEWNEW_r-concr_t-tool-instr.txt\n",
      "782 \n",
      "\n",
      "\n",
      "topor\n",
      "NEWNEW_r-concr_t-tool-instr.txt\n",
      "364 острый_DET нож_PROPN и_CCONJ крепкий_PROPN придавать_VERB уверенность_NOUN в_ADP любой_ADJ ситуация_PROPN ситуация_PROPN\n",
      "1537 \n",
      "\n",
      "\n",
      "venik\n",
      "NEWNEW_r-concr_t-tool-instr.txt\n",
      "530 \n",
      "\n",
      "\n",
      "veslo\n",
      "NEWNEW_r-concr_t-tool-instr_pt-part_pc-tool-transp_pc-tool-transp.txt\n",
      "478 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# проверяем, что в каждой строке есть целевое слово\n",
    "# для инструментов\n",
    "\n",
    "for key in keys1:\n",
    "    print(key)\n",
    "    files = os.listdir(path_to_instrumenty + key + \"\\\\\")\n",
    "    word_we_look_for = translit_instruments[key]\n",
    "    for file in files:\n",
    "        if file.startswith(\"NEWNEW_\"):\n",
    "            print(file)\n",
    "            j = 0\n",
    "            with codecs.open(path_to_instrumenty + key + \"\\\\\" + file, \"r\", \"utf-8\") as file_to_prove:\n",
    "                lines = file_to_prove.read().split(\"\\n\")\n",
    "                for i, line in enumerate(lines):\n",
    "                    if j < 10:\n",
    "                        if ((word_we_look_for+\"_NOUN\") not in line.split()):\n",
    "                            print(i, line)\n",
    "                            j += 1\n",
    "        print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Всё равно остаются какие-то неправильные, но их мало, просто при вычислении векторов будем проверять, что в строке есть интересующее нас слово"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Здесь обрабатываем \"плохие\" (=неоднозначные) контексты - лемматизируем и присваиваем ЧР"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\boss\\\\Documents\\\\Diploma\\\\RNC_Subcorpus\\\\!raznoje\\\\raw_texts\\\\without_punctuation\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading the model...\n"
     ]
    }
   ],
   "source": [
    "modelfile='C:\\\\Users\\\\boss\\\\Documents\\\\Diploma\\\\RNC_Subcorpus\\\\udpipe_syntagrus.model'\n",
    "print('\\nLoading the model...', file=sys.stderr)\n",
    "model = Model.load(modelfile)\n",
    "process_pipeline = Pipeline(model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\"glava\", \"luk\", \"organ\", \"vid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing input...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r-concr_t-text_pt-part_pc-text',_'r-concr_pt-partb_pc-hum'}.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing input...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r-concr_t-tool-weapon_top-arc',_'r-concr_t-food_pt-aggr_sc-part(plant)'}.txt\n",
      "{'r-concr_t-org_hi-class',_'r-concr_pt-partb_pc-hum_pc-animal_hi-class'}.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing input...\n",
      "Processing input...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r-abstr',_'r-abstr_der-v'}.txt\n"
     ]
    }
   ],
   "source": [
    "# лемматизируем\n",
    "output_path = \"lemmatized_bad\\\\\"\n",
    "for key in keys:\n",
    "    input_path = key + \"\\\\bad\\\\\"\n",
    "\n",
    "    input_files = os.listdir(path + input_path)\n",
    "    for file in input_files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            outputfile = path + output_path + key + \"_bad.txt\"\n",
    "            with codecs.open(path + input_path + file, 'r', encoding='utf-8') as input_f:\n",
    "                text = input_f.read() \n",
    "                # в \"разном\" убираем подчёркивания целевого слова: так не очень хорошо лемматизируется \n",
    "                text = text.replace(\"_\", \"\")\n",
    "                tag_ud(text, outputfile, process_pipeline)\n",
    "                print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# исправляем ошибки лемматизации\n",
    "translit = {\"dom\":\"Дом\",\n",
    "\"glava\":\"Глава\",\n",
    "\"luk\":\"Лук\",\n",
    "\"organ\":\"Орган\",\n",
    "\"vid\":\"Вид\"}\n",
    "\n",
    "for key in keys:\n",
    "    file_path = path + output_path + key + \"_bad.txt\"\n",
    "    with codecs.open(file_path, \"r\", \"utf-8\") as file_with_errors:\n",
    "        lines = file_with_errors.readlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        splitted_line = line.split()\n",
    "        for j, word in enumerate(splitted_line):\n",
    "            if word.split(\"_\")[0].lower() == translit[key].lower():\n",
    "                splitted_line[j] = word.split(\"_\")[0]+\"_NOUN\"\n",
    "        lines[i] = \" \".join(splitted_line)\n",
    "    with codecs.open(path + output_path + \"without_errors\\\\\" + key + \"_bad.txt\", \"a\", \"utf-8\") as file_without_errors:\n",
    "        file_without_errors.write(\"\\n\".join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glava vid_bad.txt\n",
      "'все_DET перемешать_VERB'\n",
      "glava vid_bad.txt\n",
      "''\n",
      "luk vid_bad.txt\n",
      "'все_DET перемешать_VERB'\n",
      "luk vid_bad.txt\n",
      "''\n",
      "organ vid_bad.txt\n",
      "'все_DET перемешать_VERB'\n",
      "organ vid_bad.txt\n",
      "''\n",
      "vid vid_bad.txt\n",
      "'все_DET перемешать_VERB'\n",
      "vid vid_bad.txt\n",
      "''\n"
     ]
    }
   ],
   "source": [
    "# удаляем функциональные слова\n",
    "for key in keys:\n",
    "    files = os.listdir(path + output_path + \"without_errors\\\\\")\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            with codecs.open(path + output_path + \"without_errors\\\\\" + file, \"r\", \"utf-8\") as input_f:\n",
    "                with codecs.open(path + output_path + \"without_some_POS\\\\\" + file, \"w\", \"utf-8\") as output_f: \n",
    "                    lines = input_f.read().split(\"\\n\")\n",
    "                    new_lines = []\n",
    "                    for line in lines:\n",
    "                        try:\n",
    "                            line = line[line.index(\"_NUM\")+5:]  # обрезаем номера строк\n",
    "                        except:\n",
    "                            # строки, в которых нет номера\n",
    "                            print(key, file)\n",
    "                            print(repr(line))\n",
    "                        new_line = []\n",
    "                        for word in line.split():\n",
    "                            if (word.endswith(\"_PART\") or word.endswith(\"_SCONJ\") or word.endswith(\"_CCONJ\") or \\\n",
    "                                word.endswith(\"_X\") or word.endswith(\"_ADP\") or word.endswith(\"_INTJ\") or \\\n",
    "                                word.endswith(\"_SYM\") or word.endswith(\"_AUX\")):\n",
    "                                pass\n",
    "                            else:\n",
    "                                new_line.append(word)\n",
    "                        new_lines.append(\" \".join(new_line))\n",
    "                    output_f.write(\"\\n\".join(new_lines))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
