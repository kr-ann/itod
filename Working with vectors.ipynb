{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\boss\\Anaconda3\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gensim, logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выделение контекстов, нахождение векторов для них"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-25 02:34:16,665 : INFO : loading projection weights from <zipfile.ZipExtFile name='model.bin' mode='r' compress_type=deflate>\n",
      "2019-04-25 02:34:16,667 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-04-25 02:34:25,544 : INFO : loaded (189193, 300) matrix from <zipfile.ZipExtFile [closed]>\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "model_file = \"C:\\\\Users\\\\boss\\\\Documents\\\\Diploma\\\\180.zip\"\n",
    "with zipfile.ZipFile(model_file, 'r') as archive:\n",
    "    stream = archive.open('model.bin')\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\"dom\", \"glava\", \"luk\", \"organ\", \"vid\"]\n",
    "path = \"C:\\\\Users\\\\boss\\\\Documents\\\\Diploma\\\\RNC_Subcorpus\\\\!raznoje\\\\preprocessed\\\\without_some_POS\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "translit_raznoje = {\"dom\":\"дом\",\n",
    "\"glava\":\"глава\",\n",
    "\"luk\":\"лук\",\n",
    "\"organ\":\"орган\",\n",
    "\"vid\":\"вид\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Iterable\n",
    "\n",
    "def flatten_list(items):\n",
    "    \"\"\"Yield items from any nested iterable; see Reference.\"\"\"\n",
    "    for x in items:\n",
    "        if isinstance(x, Iterable) and not isinstance(x, (str, bytes)):\n",
    "            for sub_x in flatten_list(x):\n",
    "                yield sub_x\n",
    "        else:\n",
    "            yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# класс для контекстов\n",
    "class ContextClass(object):\n",
    "    \n",
    "    def __init__(self, context, sem_set, target_word):\n",
    "        self.context = context\n",
    "        self.sem_set = sem_set\n",
    "        self.target_word = target_word\n",
    "        self.vector = None  # пока что \n",
    "    \n",
    "    def __repr__(self):\n",
    "        list_to_return = [self.sem_set, self.target_word, str(self.context)]\n",
    "        return \" - \".join(list_to_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получаем список контекстов\n",
    "\n",
    "contexts_dict = dict()\n",
    "windows_size = 5\n",
    "vector_dimensions = 300\n",
    "\n",
    "for key in translit_raznoje:\n",
    "    # сначала просто список контекстов\n",
    "    contexts_dict[key] = []\n",
    "    target_word = translit_raznoje[key] + \"_NOUN\"\n",
    "    files = os.listdir(path + key + \"\\\\\")\n",
    "    for file in files:\n",
    "        sems = file[4:-4]  # without the \"NEW_\" and \".txt\"\n",
    "        with codecs.open(path + key + \"\\\\\" + file, \"r\", \"utf-8\") as input_file:\n",
    "            lines = input_file.read().split(\"\\n\")\n",
    "        for line in lines:\n",
    "            if len(line.split()) < 3:  # совсем короткие строки нам не нужны\n",
    "                continue\n",
    "            else:\n",
    "                contexts = get_contexts(target_word, windows_size, line)\n",
    "                for context in contexts:\n",
    "                    contexts_dict[key].append(ContextClass(context, sems, target_word))\n",
    "                    \n",
    "with codecs.open(path + \"DICT_contexts.txt\", \"w\", \"utf-8\") as output_dict:\n",
    "    output_dict.write(str(contexts_dict))  # записали, потому что вектора всё равно там не отражаются"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# теперь добавляем вектора\n",
    "for key in contexts_dict:\n",
    "    for i, context_instance in enumerate(contexts_dict[key]):\n",
    "        contexts_dict[key][i].vector = get_context_vector(context_instance.context, windows_size, vector_dimensions, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contexts(target_word, windows_size, line):\n",
    "    # line is with POS tags, so is the target word (дом_NOUN)\n",
    "    list_of_contexts = []\n",
    "    \n",
    "    splitted_line = line.split()\n",
    "    for i, word in enumerate(splitted_line):\n",
    "        if word == target_word:\n",
    "            context = []\n",
    "            \n",
    "            # проверка слева\n",
    "            for counter in range(windows_size):\n",
    "                index = i - windows_size + counter\n",
    "                if index < 0:\n",
    "                    # добаляем слева инстансы, которые в последствии дадут пустые вектора\n",
    "                    context.append(\"_#_#_#_\")  # something that is definetely not in the model and will return an empty vector\n",
    "                else:\n",
    "                    context.append(splitted_line[index])\n",
    "            \n",
    "            context.append(splitted_line[i])\n",
    "                           \n",
    "            # теперь проверяем правую сторону\n",
    "            for counter in range(windows_size):\n",
    "                try:\n",
    "                    context.append(splitted_line[i + counter + 1])\n",
    "                except:\n",
    "                    context.append(\"_#_#_#_\")\n",
    "            list_of_contexts.append(context) \n",
    "                \n",
    "    return(list_of_contexts)      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# все контексты в таком виде, что целевое слово посередине, а если слов слева/справа не хватает, там стоит \n",
    "# последовательность \"_#_#_#_\", которая даст пустые вектора (тк её точно нет в модели)\n",
    "\n",
    "def get_context_vector(context, window_size, vector_dimensions, model):\n",
    "    words_vectors = []\n",
    "    context_vector = numpy.zeros(vector_dimensions)\n",
    "    \n",
    "    if len(context) != window_size * 2 + 1:\n",
    "        raise ValueError(\"Контекст неправильной размерности\")\n",
    "\n",
    "    for word in context:\n",
    "        if (word in model):\n",
    "            words_vectors.append(model[word])\n",
    "        else: \n",
    "            words_vectors.append(numpy.zeros(vector_dimensions))\n",
    "\n",
    "\n",
    "    for i in range(300):\n",
    "        for j, vector in enumerate(words_vectors):\n",
    "            if j != window_size + 1:\n",
    "                context_vector[i] += vector[i] * ((window_size - math.fabs(window_size + 1 - j))/window_size)\n",
    "    return(context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# примерчик\n",
    "sample_context = [\"ребенок_NOUN\", \"страдать_VERB\", \"врожденный_ADJ\", \"порок_NOUN\", \"различный_ADJ\", \n",
    "                  \"орган_NOUN\", \n",
    "                  \"новообразование_NOUN\", \"другой_ADJ\", \"тяжелый_ADJ\", \"недуг_NOUN\", \"можно_ADV\"]\n",
    "res = get_context_vector(sample_context, 5, 300, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Классификация! "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "В словаре contexts_dict по ключам-названиям слов находятся инстансы класса ContextClass, у которых\n",
    "        self.context - словарь длиной 11, где шестое слово - целевое, а вокруг другие слова\n",
    "        self.sem_set - название файла, из которого он взят\n",
    "        self.target_word - целевое слово в формате \"слово_NOUN\"\n",
    "        self.vector - вектор, посчитанный для контекста"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Кластеризуем вектора (количество кластеров - сколько у нас значений), смотрим, насколько хорошо кластеры соответствуют значениям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.cluster import KMeansClusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dom\n",
      "8\n",
      "glava\n",
      "4\n",
      "luk\n",
      "2\n",
      "organ\n",
      "4\n",
      "vid\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# словарь, значение которого для каждого целевого слова - зип файл с парами (сема, присвоенный номер кластера) \n",
    "dict_with_cluster_sem_correspondence = dict()\n",
    "\n",
    "for key in contexts_dict:\n",
    "    files = os.listdir(path + key + \"\\\\\")\n",
    "    NUM_CLUSTERS = len(files)\n",
    "    print(key)\n",
    "    print(NUM_CLUSTERS)\n",
    "    list_with_vectors = [instance.vector for instance in contexts_dict[key]]  # все векторы\n",
    "    list_with_sems = [instance.sem_set for instance in contexts_dict[key]]  # все семы\n",
    "    kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "    list_with_cluster_labels = kclusterer.cluster(list_with_vectors, assign_clusters=True)\n",
    "    dict_with_cluster_sem_correspondence[key] = zip(list_with_sems, list_with_cluster_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dict_with_cluster_sem_correspondence:\n",
    "    dict_with_cluster_sem_correspondence[key] = list(dict_with_cluster_sem_correspondence[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"{'r-concr_t-constr_top-contain',_'r-concr_t-org'}\", 5), (\"{'r-concr_t-constr_top-contain',_'r-concr_t-org'}\", 5), (\"{'r-concr_t-constr_top-contain',_'r-concr_t-org'}\", 6), (\"{'r-concr_t-constr_top-contain',_'r-concr_t-org'}\", 5), (\"{'r-concr_t-constr_top-contain',_'r-concr_t-org'}\", 5), (\"{'r-concr_t-constr_top-contain',_'r-concr_t-org'}\", 4), (\"{'r-concr_t-constr_top-contain',_'r-concr_t-org'}\", 5), (\"{'r-concr_t-constr_top-contain',_'r-concr_t-org'}\", 4), (\"{'r-concr_t-constr_top-contain',_'r-concr_t-org'}\", 5), (\"{'r-concr_t-constr_top-contain',_'r-concr_t-org'}\", 5)]\n",
      "[(\"{'r-concr_der-shift_dt-partb'}\", 3), (\"{'r-concr_der-shift_dt-partb'}\", 2), (\"{'r-concr_der-shift_dt-partb'}\", 2), (\"{'r-concr_der-shift_dt-partb'}\", 3), (\"{'r-concr_der-shift_dt-partb'}\", 2), (\"{'r-concr_der-shift_dt-partb'}\", 1), (\"{'r-concr_der-shift_dt-partb'}\", 1), (\"{'r-concr_der-shift_dt-partb'}\", 3), (\"{'r-concr_der-shift_dt-partb'}\", 2), (\"{'r-concr_der-shift_dt-partb'}\", 1)]\n",
      "[(\"{'r-concr_t-plant_t-fruit_t-food_pt-aggr'}\", 1), (\"{'r-concr_t-plant_t-fruit_t-food_pt-aggr'}\", 1), (\"{'r-concr_t-plant_t-fruit_t-food_pt-aggr'}\", 1), (\"{'r-concr_t-plant_t-fruit_t-food_pt-aggr'}\", 1), (\"{'r-concr_t-plant_t-fruit_t-food_pt-aggr'}\", 1), (\"{'r-concr_t-plant_t-fruit_t-food_pt-aggr'}\", 1), (\"{'r-concr_t-plant_t-fruit_t-food_pt-aggr'}\", 1), (\"{'r-concr_t-plant_t-fruit_t-food_pt-aggr'}\", 1), (\"{'r-concr_t-plant_t-fruit_t-food_pt-aggr'}\", 1), (\"{'r-concr_t-plant_t-fruit_t-food_pt-aggr'}\", 1)]\n",
      "[(\"{'r-concr_der-shift_dt-partb'}\", 1), (\"{'r-concr_der-shift_dt-partb'}\", 3), (\"{'r-concr_der-shift_dt-partb'}\", 0), (\"{'r-concr_der-shift_dt-partb'}\", 0), (\"{'r-concr_der-shift_dt-partb'}\", 0), (\"{'r-concr_der-shift_dt-partb'}\", 1), (\"{'r-concr_der-shift_dt-partb'}\", 0), (\"{'r-concr_der-shift_dt-partb'}\", 1), (\"{'r-concr_der-shift_dt-partb'}\", 1), (\"{'r-concr_der-shift_dt-partb'}\", 1)]\n",
      "[(\"{'r-abstr_der-shift'}\", 1), (\"{'r-abstr_der-shift'}\", 5), (\"{'r-abstr_der-shift'}\", 1), (\"{'r-abstr_der-shift'}\", 1), (\"{'r-abstr_der-shift'}\", 1), (\"{'r-abstr_der-shift'}\", 3), (\"{'r-abstr_der-shift'}\", 1), (\"{'r-abstr_der-shift'}\", 5), (\"{'r-abstr_der-shift'}\", 0), (\"{'r-abstr_der-shift'}\", 1)]\n"
     ]
    }
   ],
   "source": [
    "for key in dict_with_cluster_sem_correspondence:\n",
    "    print(dict_with_cluster_sem_correspondence[key][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dom\n",
      "3629\n",
      "glava\n",
      "733\n",
      "luk\n",
      "2642\n",
      "organ\n",
      "1136\n",
      "vid\n",
      "3033\n"
     ]
    }
   ],
   "source": [
    "for key in dict_with_cluster_sem_correspondence:\n",
    "    print(key)\n",
    "    print(len(dict_with_cluster_sem_correspondence[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dom\n",
      "2\n",
      "glava\n",
      "2\n",
      "luk\n",
      "2\n",
      "organ\n",
      "2\n",
      "vid\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for key in dict_with_cluster_sem_correspondence:\n",
    "    print(key)\n",
    "    print(len(dict_with_cluster_sem_correspondence[key][5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dom\n",
      "Всего правильных контекстов\n",
      "[(\"{'r-concr_t-constr_top-contain',_'r-concr_t-org'}\", 269), (\"{'r-concr_t-constr_top-contain',_'r-concr_t-space'}\", 1003), (\"{'r-concr_t-constr_top-contain'}\", 2105), (\"{'r-concr_t-group_pt-set_sc-hum',_'r-concr_t-constr_top-contain',_'r-concr_t-space'}\", 46), (\"{'r-concr_t-group_pt-set_sc-hum'}\", 23), (\"{'r-concr_t-org',_'r-concr_t-constr_top-contain',_'r-concr_t-space'}\", 71), (\"{'r-concr_t-org'}\", 13), (\"{'r-concr_t-space'}\", 99)]\n",
      "glava\n",
      "Всего правильных контекстов\n",
      "[(\"{'r-concr_der-shift_dt-partb'}\", 139), (\"{'r-concr_pt-partb_pc-hum'}\", 8), (\"{'r-concr_t-hum'}\", 311), (\"{'r-concr_t-text_pt-part_pc-text'}\", 275)]\n",
      "luk\n",
      "Всего правильных контекстов\n",
      "[(\"{'r-concr_t-plant_t-fruit_t-food_pt-aggr'}\", 1979), (\"{'r-concr_t-tool-weapon_top-arc'}\", 663)]\n",
      "organ\n",
      "Всего правильных контекстов\n",
      "[(\"{'r-concr_der-shift_dt-partb'}\", 12), (\"{'r-concr_pt-partb_pc-hum_pc-animal_hi-class'}\", 170), (\"{'r-concr_t-org_hi-class'}\", 924), (\"{'r-concr_t-tool-mus'}\", 30)]\n",
      "vid\n",
      "Всего правильных контекстов\n",
      "[(\"{'r-abstr_der-shift'}\", 1154), (\"{'r-abstr_r-concr_pt-set_sc-X'}\", 656), (\"{'r-abstr_t-ment'}\", 13), (\"{'r-abstr_t-perc_der-v'}\", 1193), (\"{'r-concr_t-doc'}\", 7), (\"{'r-concr_t-workart'}\", 10)]\n"
     ]
    }
   ],
   "source": [
    "# смотрим, сколько в исходных файлых контекстов на каждое значение\n",
    "for key in dict_with_cluster_sem_correspondence:\n",
    "    files = os.listdir(path + key + \"\\\\\")\n",
    "    senses = []\n",
    "    for file in files:\n",
    "        senses.append(file[4:-4])\n",
    "    \n",
    "    num_true_contexts = [0]*len(senses)\n",
    "    for i, sense in enumerate(senses):\n",
    "        for pair in dict_with_cluster_sem_correspondence[key]:\n",
    "            if pair[0] == sense:\n",
    "                num_true_contexts[i] += 1\n",
    "    print(key)\n",
    "    print(\"Всего правильных контекстов\")\n",
    "    print(list(zip(senses, num_true_contexts)))\n",
    "        \n",
    "    \n",
    "    #print(list(dict_with_cluster_sem_correspondence[key])[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# новый словарь - значения - тоже словари, где ключами в свою очередь являются различные смыслы целевого слова, а "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dom\n",
      "{'r-concr_t-constr_top-contain',_'r-concr_t-org'} 269\n",
      "{'r-concr_t-constr_top-contain',_'r-concr_t-space'} 1003\n",
      "{'r-concr_t-constr_top-contain'} 2105\n",
      "{'r-concr_t-group_pt-set_sc-hum',_'r-concr_t-constr_top-contain',_'r-concr_t-space'} 46\n",
      "{'r-concr_t-group_pt-set_sc-hum'} 23\n",
      "{'r-concr_t-org',_'r-concr_t-constr_top-contain',_'r-concr_t-space'} 71\n",
      "{'r-concr_t-org'} 13\n",
      "{'r-concr_t-space'} 99\n",
      "glava\n",
      "{'r-concr_der-shift_dt-partb'} 139\n",
      "{'r-concr_pt-partb_pc-hum'} 8\n",
      "{'r-concr_t-hum'} 311\n",
      "{'r-concr_t-text_pt-part_pc-text'} 275\n",
      "luk\n",
      "{'r-concr_t-plant_t-fruit_t-food_pt-aggr'} 1979\n",
      "{'r-concr_t-tool-weapon_top-arc'} 663\n",
      "organ\n",
      "{'r-concr_der-shift_dt-partb'} 12\n",
      "{'r-concr_pt-partb_pc-hum_pc-animal_hi-class'} 170\n",
      "{'r-concr_t-org_hi-class'} 924\n",
      "{'r-concr_t-tool-mus'} 30\n",
      "vid\n",
      "{'r-abstr_der-shift'} 1154\n",
      "{'r-abstr_r-concr_pt-set_sc-X'} 656\n",
      "{'r-abstr_t-ment'} 13\n",
      "{'r-abstr_t-perc_der-v'} 1193\n",
      "{'r-concr_t-doc'} 7\n",
      "{'r-concr_t-workart'} 10\n"
     ]
    }
   ],
   "source": [
    "new_dict_with_all_assigned_clusters = dict()\n",
    "\n",
    "for key in dict_with_cluster_sem_correspondence:\n",
    "    new_dict_with_all_assigned_clusters[key] = dict()\n",
    "    for pair in dict_with_cluster_sem_correspondence[key]:\n",
    "        new_dict_with_all_assigned_clusters[key].setdefault(pair[0], [])\n",
    "        new_dict_with_all_assigned_clusters[key][pair[0]].append(pair[1])\n",
    "\n",
    "# посмотрели, что здесь всего столько же контекстов получается - уже хорошо\n",
    "for key in new_dict_with_all_assigned_clusters:\n",
    "    print(key)\n",
    "    for sema in new_dict_with_all_assigned_clusters[key]:\n",
    "        print(sema, len(new_dict_with_all_assigned_clusters[key][sema]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dom\n",
      "{'r-concr_t-constr_top-contain',_'r-concr_t-org'}\n",
      "[5, 5, 6, 5, 5, 4, 5, 4, 5, 5]\n",
      "{'r-concr_t-constr_top-contain',_'r-concr_t-space'}\n",
      "[1, 5, 7, 7, 4, 4, 3, 6, 0, 0]\n",
      "{'r-concr_t-constr_top-contain'}\n",
      "[1, 2, 1, 2, 3, 2, 0, 7, 5, 1]\n",
      "{'r-concr_t-group_pt-set_sc-hum',_'r-concr_t-constr_top-contain',_'r-concr_t-space'}\n",
      "[3, 3, 4, 3, 5, 6, 4, 4, 1, 1]\n",
      "{'r-concr_t-group_pt-set_sc-hum'}\n",
      "[3, 7, 4, 4, 1, 5, 4, 6, 3, 7]\n",
      "{'r-concr_t-org',_'r-concr_t-constr_top-contain',_'r-concr_t-space'}\n",
      "[5, 5, 4, 3, 3, 3, 6, 4, 4, 5]\n",
      "{'r-concr_t-org'}\n",
      "[5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "{'r-concr_t-space'}\n",
      "[0, 3, 3, 5, 1, 6, 5, 3, 3, 3]\n",
      "glava\n",
      "{'r-concr_der-shift_dt-partb'}\n",
      "[3, 2, 2, 3, 2, 1, 1, 3, 2, 1]\n",
      "{'r-concr_pt-partb_pc-hum'}\n",
      "[1, 1, 1, 1, 0, 1, 0, 1]\n",
      "{'r-concr_t-hum'}\n",
      "[0, 3, 3, 2, 0, 3, 2, 3, 0, 1]\n",
      "{'r-concr_t-text_pt-part_pc-text'}\n",
      "[1, 3, 3, 3, 3, 3, 3, 3, 3, 1]\n",
      "luk\n",
      "{'r-concr_t-plant_t-fruit_t-food_pt-aggr'}\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "{'r-concr_t-tool-weapon_top-arc'}\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "organ\n",
      "{'r-concr_der-shift_dt-partb'}\n",
      "[1, 3, 0, 0, 0, 1, 0, 1, 1, 1]\n",
      "{'r-concr_pt-partb_pc-hum_pc-animal_hi-class'}\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "{'r-concr_t-org_hi-class'}\n",
      "[2, 2, 2, 1, 0, 2, 2, 3, 2, 0]\n",
      "{'r-concr_t-tool-mus'}\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "vid\n",
      "{'r-abstr_der-shift'}\n",
      "[1, 5, 1, 1, 1, 3, 1, 5, 0, 1]\n",
      "{'r-abstr_r-concr_pt-set_sc-X'}\n",
      "[3, 1, 1, 1, 1, 1, 1, 4, 1, 1]\n",
      "{'r-abstr_t-ment'}\n",
      "[1, 3, 3, 5, 3, 3, 3, 3, 2, 3]\n",
      "{'r-abstr_t-perc_der-v'}\n",
      "[3, 1, 2, 2, 3, 5, 3, 2, 0, 4]\n",
      "{'r-concr_t-doc'}\n",
      "[3, 1, 1, 5, 3, 0, 4]\n",
      "{'r-concr_t-workart'}\n",
      "[4, 1, 0, 0, 4, 0, 0, 3, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "for key in new_dict_with_all_assigned_clusters:\n",
    "    print(key)\n",
    "    for sema in new_dict_with_all_assigned_clusters[key]:\n",
    "        print(sema)\n",
    "        print(new_dict_with_all_assigned_clusters[key][sema][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dom\n",
      "{'r-concr_t-constr_top-contain',_'r-concr_t-org'}\n",
      "5 77\n",
      "6 40\n",
      "4 81\n",
      "2 21\n",
      "3 14\n",
      "0 14\n",
      "1 11\n",
      "7 11\n",
      "{'r-concr_t-constr_top-contain',_'r-concr_t-space'}\n",
      "1 90\n",
      "5 71\n",
      "7 140\n",
      "4 215\n",
      "3 238\n",
      "6 170\n",
      "0 46\n",
      "2 33\n",
      "{'r-concr_t-constr_top-contain'}\n",
      "1 270\n",
      "2 366\n",
      "3 208\n",
      "0 140\n",
      "7 198\n",
      "5 218\n",
      "4 526\n",
      "6 179\n",
      "{'r-concr_t-group_pt-set_sc-hum',_'r-concr_t-constr_top-contain',_'r-concr_t-space'}\n",
      "3 10\n",
      "4 13\n",
      "5 4\n",
      "6 4\n",
      "1 6\n",
      "7 8\n",
      "2 1\n",
      "{'r-concr_t-group_pt-set_sc-hum'}\n",
      "3 4\n",
      "7 3\n",
      "4 8\n",
      "1 3\n",
      "5 2\n",
      "6 3\n",
      "{'r-concr_t-org',_'r-concr_t-constr_top-contain',_'r-concr_t-space'}\n",
      "5 12\n",
      "4 19\n",
      "3 10\n",
      "6 7\n",
      "7 6\n",
      "2 9\n",
      "1 6\n",
      "0 2\n",
      "{'r-concr_t-org'}\n",
      "5 11\n",
      "4 1\n",
      "2 1\n",
      "{'r-concr_t-space'}\n",
      "0 11\n",
      "3 28\n",
      "5 9\n",
      "1 3\n",
      "6 16\n",
      "4 12\n",
      "7 19\n",
      "2 1\n",
      "glava\n",
      "{'r-concr_der-shift_dt-partb'}\n",
      "3 25\n",
      "2 43\n",
      "1 53\n",
      "0 18\n",
      "{'r-concr_pt-partb_pc-hum'}\n",
      "1 6\n",
      "0 2\n",
      "{'r-concr_t-hum'}\n",
      "0 71\n",
      "3 59\n",
      "2 153\n",
      "1 28\n",
      "{'r-concr_t-text_pt-part_pc-text'}\n",
      "1 83\n",
      "3 71\n",
      "0 109\n",
      "2 12\n",
      "luk\n",
      "{'r-concr_t-plant_t-fruit_t-food_pt-aggr'}\n",
      "1 1780\n",
      "0 199\n",
      "{'r-concr_t-tool-weapon_top-arc'}\n",
      "1 654\n",
      "0 9\n",
      "organ\n",
      "{'r-concr_der-shift_dt-partb'}\n",
      "1 6\n",
      "3 1\n",
      "0 5\n",
      "{'r-concr_pt-partb_pc-hum_pc-animal_hi-class'}\n",
      "1 128\n",
      "0 38\n",
      "3 2\n",
      "2 2\n",
      "{'r-concr_t-org_hi-class'}\n",
      "2 236\n",
      "1 123\n",
      "0 265\n",
      "3 300\n",
      "{'r-concr_t-tool-mus'}\n",
      "1 26\n",
      "0 4\n",
      "vid\n",
      "{'r-abstr_der-shift'}\n",
      "1 227\n",
      "5 308\n",
      "3 285\n",
      "0 162\n",
      "4 135\n",
      "2 37\n",
      "{'r-abstr_r-concr_pt-set_sc-X'}\n",
      "3 244\n",
      "1 264\n",
      "4 34\n",
      "0 87\n",
      "2 14\n",
      "5 13\n",
      "{'r-abstr_t-ment'}\n",
      "1 1\n",
      "3 9\n",
      "5 2\n",
      "2 1\n",
      "{'r-abstr_t-perc_der-v'}\n",
      "3 217\n",
      "1 32\n",
      "2 251\n",
      "5 32\n",
      "0 160\n",
      "4 501\n",
      "{'r-concr_t-doc'}\n",
      "3 2\n",
      "1 2\n",
      "5 1\n",
      "0 1\n",
      "4 1\n",
      "{'r-concr_t-workart'}\n",
      "4 2\n",
      "1 1\n",
      "0 6\n",
      "3 1\n"
     ]
    }
   ],
   "source": [
    "for key in new_dict_with_all_assigned_clusters:\n",
    "    print(key)\n",
    "    for sema in new_dict_with_all_assigned_clusters[key]:\n",
    "        print(sema)\n",
    "        small_dict = dict()\n",
    "        for number in new_dict_with_all_assigned_clusters[key][sema]:\n",
    "            small_dict.setdefault(number, 0)\n",
    "            small_dict[number] += 1\n",
    "        for element in small_dict:\n",
    "            print(element, small_dict[element])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
