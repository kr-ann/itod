{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\boss\\Anaconda3\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gensim, logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выделение контекстов, нахождение векторов для них"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-25 02:34:16,665 : INFO : loading projection weights from <zipfile.ZipExtFile name='model.bin' mode='r' compress_type=deflate>\n",
      "2019-04-25 02:34:16,667 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-04-25 02:34:25,544 : INFO : loaded (189193, 300) matrix from <zipfile.ZipExtFile [closed]>\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "model_file = \"C:\\\\Users\\\\boss\\\\Documents\\\\Diploma\\\\180.zip\"\n",
    "with zipfile.ZipFile(model_file, 'r') as archive:\n",
    "    stream = archive.open('model.bin')\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\"dom\", \"glava\", \"luk\", \"organ\", \"vid\"]\n",
    "path = \"C:\\\\Users\\\\boss\\\\Documents\\\\Diploma\\\\RNC_Subcorpus\\\\!raznoje\\\\preprocessed\\\\without_some_POS\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "translit_raznoje = {\"dom\":\"дом\",\n",
    "\"glava\":\"глава\",\n",
    "\"luk\":\"лук\",\n",
    "\"organ\":\"орган\",\n",
    "\"vid\":\"вид\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Iterable\n",
    "\n",
    "def flatten_list(items):\n",
    "    \"\"\"Yield items from any nested iterable; see Reference.\"\"\"\n",
    "    for x in items:\n",
    "        if isinstance(x, Iterable) and not isinstance(x, (str, bytes)):\n",
    "            for sub_x in flatten_list(x):\n",
    "                yield sub_x\n",
    "        else:\n",
    "            yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# класс для контекстов\n",
    "class ContextClass(object):\n",
    "    \n",
    "    def __init__(self, context, sem_set, target_word):\n",
    "        self.context = context\n",
    "        self.sem_set = sem_set\n",
    "        self.target_word = target_word\n",
    "        self.vector = None  # пока что \n",
    "    \n",
    "    def __repr__(self):\n",
    "        list_to_return = [self.sem_set, self.target_word, str(self.context)]\n",
    "        return \" - \".join(list_to_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получаем список контекстов\n",
    "\n",
    "contexts_dict = dict()\n",
    "windows_size = 5\n",
    "vector_dimensions = 300\n",
    "\n",
    "for key in translit_raznoje:\n",
    "    # сначала просто список контекстов\n",
    "    contexts_dict[key] = []\n",
    "    target_word = translit_raznoje[key] + \"_NOUN\"\n",
    "    files = os.listdir(path + key + \"\\\\\")\n",
    "    for file in files:\n",
    "        sems = file[4:-4]  # without the \"NEW_\" and \".txt\"\n",
    "        with codecs.open(path + key + \"\\\\\" + file, \"r\", \"utf-8\") as input_file:\n",
    "            lines = input_file.read().split(\"\\n\")\n",
    "        for line in lines:\n",
    "            if len(line.split()) < 3:  # совсем короткие строки нам не нужны\n",
    "                continue\n",
    "            else:\n",
    "                contexts = get_contexts(target_word, windows_size, line)\n",
    "                for context in contexts:\n",
    "                    contexts_dict[key].append(ContextClass(context, sems, target_word))\n",
    "                    \n",
    "with codecs.open(path + \"DICT_contexts.txt\", \"w\", \"utf-8\") as output_dict:\n",
    "    output_dict.write(str(contexts_dict))  # записали, потому что вектора всё равно там не отражаются"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# теперь добавляем вектора\n",
    "for key in contexts_dict:\n",
    "    for i, context_instance in enumerate(contexts_dict[key]):\n",
    "        contexts_dict[key][i].vector = get_context_vector(context_instance.context, windows_size, vector_dimensions, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contexts(target_word, windows_size, line):\n",
    "    # line is with POS tags, so is the target word (дом_NOUN)\n",
    "    list_of_contexts = []\n",
    "    \n",
    "    splitted_line = line.split()\n",
    "    for i, word in enumerate(splitted_line):\n",
    "        if word == target_word:\n",
    "            context = []\n",
    "            \n",
    "            # проверка слева\n",
    "            for counter in range(windows_size):\n",
    "                index = i - windows_size + counter\n",
    "                if index < 0:\n",
    "                    # добаляем слева инстансы, которые в последствии дадут пустые вектора\n",
    "                    context.append(\"_#_#_#_\")  # something that is definetely not in the model and will return an empty vector\n",
    "                else:\n",
    "                    context.append(splitted_line[index])\n",
    "            \n",
    "            context.append(splitted_line[i])\n",
    "                           \n",
    "            # теперь проверяем правую сторону\n",
    "            for counter in range(windows_size):\n",
    "                try:\n",
    "                    context.append(splitted_line[i + counter + 1])\n",
    "                except:\n",
    "                    context.append(\"_#_#_#_\")\n",
    "            list_of_contexts.append(context) \n",
    "                \n",
    "    return(list_of_contexts)      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# все контексты в таком виде, что целевое слово посередине, а если слов слева/справа не хватает, там стоит \n",
    "# последовательность \"_#_#_#_\", которая даст пустые вектора (тк её точно нет в модели)\n",
    "\n",
    "def get_context_vector(context, window_size, vector_dimensions, model):\n",
    "    words_vectors = []\n",
    "    context_vector = numpy.zeros(vector_dimensions)\n",
    "    \n",
    "    if len(context) != window_size * 2 + 1:\n",
    "        raise ValueError(\"Контекст неправильной размерности\")\n",
    "\n",
    "    for word in context:\n",
    "        if (word in model):\n",
    "            words_vectors.append(model[word])\n",
    "        else: \n",
    "            words_vectors.append(numpy.zeros(vector_dimensions))\n",
    "\n",
    "\n",
    "    for i in range(300):\n",
    "        for j, vector in enumerate(words_vectors):\n",
    "            if j != window_size + 1:\n",
    "                context_vector[i] += vector[i] * ((window_size - math.fabs(window_size + 1 - j))/window_size)\n",
    "    return(context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# примерчик\n",
    "sample_context = [\"ребенок_NOUN\", \"страдать_VERB\", \"врожденный_ADJ\", \"порок_NOUN\", \"различный_ADJ\", \n",
    "                  \"орган_NOUN\", \n",
    "                  \"новообразование_NOUN\", \"другой_ADJ\", \"тяжелый_ADJ\", \"недуг_NOUN\", \"можно_ADV\"]\n",
    "res = get_context_vector(sample_context, 5, 300, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Классификация! "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "В словаре contexts_dict по ключам-названиям слов находятся инстансы класса ContextClass, у которых\n",
    "        self.context - словарь длиной 11, где шестое слово - целевое, а вокруг другие слова\n",
    "        self.sem_set - название файла, из которого он взят\n",
    "        self.target_word - целевое слово в формате \"слово_NOUN\"\n",
    "        self.vector - вектор, посчитанный для контекста"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Кластеризуем вектора (количество кластеров - сколько у нас значений), смотрим, насколько хорошо кластеры соответствуют значениям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.cluster import KMeansClusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_with_cluster_sem_correspondence = dict()\n",
    "\n",
    "for key in contexts_dict:\n",
    "    files = os.listdir(path + key + \"\\\\\")\n",
    "    NUM_CLUSTERS = len(files)\n",
    "    list_with_vectors = [instance.vector for instance in contexts_dict[key]]  # все векторы\n",
    "    list_with_sems = [instance.sem_set for instance in contexts_dict[key]]  # все семы\n",
    "    kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "    list_with_cluster_labels = kclusterer.cluster(list_with_vectors, assign_clusters=True)\n",
    "    dict_with_cluster_sem_correspondence[key] = zip(list_with_sems, list_with_cluster_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"{'r-concr_t-constr_top-contain',_'r-concr_t-org'}\", 6), (\"{'r-concr_t-constr_top-contain',_'r-concr_t-org'}\", 6), (\"{'r-concr_t-constr_top-contain',_'r-concr_t-org'}\", 5), (\"{'r-concr_t-constr_top-contain',_'r-concr_t-org'}\", 6), (\"{'r-concr_t-constr_top-contain',_'r-concr_t-org'}\", 6), (\"{'r-concr_t-constr_top-contain',_'r-concr_t-org'}\", 3), (\"{'r-concr_t-constr_top-contain',_'r-concr_t-org'}\", 6), (\"{'r-concr_t-constr_top-contain',_'r-concr_t-org'}\", 3), (\"{'r-concr_t-constr_top-contain',_'r-concr_t-org'}\", 6), (\"{'r-concr_t-constr_top-contain',_'r-concr_t-org'}\", 6), (\"{'r-concr_t-constr_top-contain',_'r-concr_t-org'}\", 6), (\"{'r-concr_t-constr_top-contain',_'r-concr_t-org'}\", 6), (\"{'r-concr_t-constr_top-contain',_'r-concr_t-org'}\", 1), (\"{'r-concr_t-constr_top-contain',_'r-concr_t-org'}\", 6), (\"{'r-concr_t-constr_top-contain',_'r-concr_t-org'}\", 3), (\"{'r-concr_t-constr_top-contain',_'r-concr_t-org'}\", 6), (\"{'r-concr_t-constr_top-contain',_'r-concr_t-org'}\", 6), (\"{'r-concr_t-constr_top-contain',_'r-concr_t-org'}\", 1), (\"{'r-concr_t-constr_top-contain',_'r-concr_t-org'}\", 6), (\"{'r-concr_t-constr_top-contain',_'r-concr_t-org'}\", 5)]\n",
      "[(\"{'r-concr_der-shift_dt-partb'}\", 3), (\"{'r-concr_der-shift_dt-partb'}\", 0), (\"{'r-concr_der-shift_dt-partb'}\", 0), (\"{'r-concr_der-shift_dt-partb'}\", 3), (\"{'r-concr_der-shift_dt-partb'}\", 2), (\"{'r-concr_der-shift_dt-partb'}\", 1), (\"{'r-concr_der-shift_dt-partb'}\", 0), (\"{'r-concr_der-shift_dt-partb'}\", 3), (\"{'r-concr_der-shift_dt-partb'}\", 2), (\"{'r-concr_der-shift_dt-partb'}\", 0), (\"{'r-concr_der-shift_dt-partb'}\", 3), (\"{'r-concr_der-shift_dt-partb'}\", 3), (\"{'r-concr_der-shift_dt-partb'}\", 2), (\"{'r-concr_der-shift_dt-partb'}\", 3), (\"{'r-concr_der-shift_dt-partb'}\", 0), (\"{'r-concr_der-shift_dt-partb'}\", 0), (\"{'r-concr_der-shift_dt-partb'}\", 0), (\"{'r-concr_der-shift_dt-partb'}\", 0), (\"{'r-concr_der-shift_dt-partb'}\", 0), (\"{'r-concr_der-shift_dt-partb'}\", 2)]\n",
      "[(\"{'r-concr_t-plant_t-fruit_t-food_pt-aggr'}\", 1), (\"{'r-concr_t-plant_t-fruit_t-food_pt-aggr'}\", 1), (\"{'r-concr_t-plant_t-fruit_t-food_pt-aggr'}\", 1), (\"{'r-concr_t-plant_t-fruit_t-food_pt-aggr'}\", 1), (\"{'r-concr_t-plant_t-fruit_t-food_pt-aggr'}\", 1), (\"{'r-concr_t-plant_t-fruit_t-food_pt-aggr'}\", 1), (\"{'r-concr_t-plant_t-fruit_t-food_pt-aggr'}\", 1), (\"{'r-concr_t-plant_t-fruit_t-food_pt-aggr'}\", 1), (\"{'r-concr_t-plant_t-fruit_t-food_pt-aggr'}\", 1), (\"{'r-concr_t-plant_t-fruit_t-food_pt-aggr'}\", 1), (\"{'r-concr_t-plant_t-fruit_t-food_pt-aggr'}\", 1), (\"{'r-concr_t-plant_t-fruit_t-food_pt-aggr'}\", 1), (\"{'r-concr_t-plant_t-fruit_t-food_pt-aggr'}\", 1), (\"{'r-concr_t-plant_t-fruit_t-food_pt-aggr'}\", 1), (\"{'r-concr_t-plant_t-fruit_t-food_pt-aggr'}\", 1), (\"{'r-concr_t-plant_t-fruit_t-food_pt-aggr'}\", 1), (\"{'r-concr_t-plant_t-fruit_t-food_pt-aggr'}\", 0), (\"{'r-concr_t-plant_t-fruit_t-food_pt-aggr'}\", 0), (\"{'r-concr_t-plant_t-fruit_t-food_pt-aggr'}\", 1), (\"{'r-concr_t-plant_t-fruit_t-food_pt-aggr'}\", 0)]\n",
      "[(\"{'r-concr_der-shift_dt-partb'}\", 0), (\"{'r-concr_der-shift_dt-partb'}\", 3), (\"{'r-concr_der-shift_dt-partb'}\", 1), (\"{'r-concr_der-shift_dt-partb'}\", 0), (\"{'r-concr_der-shift_dt-partb'}\", 0), (\"{'r-concr_der-shift_dt-partb'}\", 0), (\"{'r-concr_der-shift_dt-partb'}\", 1), (\"{'r-concr_der-shift_dt-partb'}\", 0), (\"{'r-concr_der-shift_dt-partb'}\", 0), (\"{'r-concr_der-shift_dt-partb'}\", 0), (\"{'r-concr_der-shift_dt-partb'}\", 0), (\"{'r-concr_der-shift_dt-partb'}\", 0), (\"{'r-concr_pt-partb_pc-hum_pc-animal_hi-class'}\", 0), (\"{'r-concr_pt-partb_pc-hum_pc-animal_hi-class'}\", 0), (\"{'r-concr_pt-partb_pc-hum_pc-animal_hi-class'}\", 0), (\"{'r-concr_pt-partb_pc-hum_pc-animal_hi-class'}\", 0), (\"{'r-concr_pt-partb_pc-hum_pc-animal_hi-class'}\", 0), (\"{'r-concr_pt-partb_pc-hum_pc-animal_hi-class'}\", 0), (\"{'r-concr_pt-partb_pc-hum_pc-animal_hi-class'}\", 0), (\"{'r-concr_pt-partb_pc-hum_pc-animal_hi-class'}\", 0)]\n",
      "[(\"{'r-abstr_der-shift'}\", 2), (\"{'r-abstr_der-shift'}\", 5), (\"{'r-abstr_der-shift'}\", 2), (\"{'r-abstr_der-shift'}\", 2), (\"{'r-abstr_der-shift'}\", 2), (\"{'r-abstr_der-shift'}\", 4), (\"{'r-abstr_der-shift'}\", 2), (\"{'r-abstr_der-shift'}\", 5), (\"{'r-abstr_der-shift'}\", 0), (\"{'r-abstr_der-shift'}\", 2), (\"{'r-abstr_der-shift'}\", 2), (\"{'r-abstr_der-shift'}\", 2), (\"{'r-abstr_der-shift'}\", 2), (\"{'r-abstr_der-shift'}\", 3), (\"{'r-abstr_der-shift'}\", 2), (\"{'r-abstr_der-shift'}\", 2), (\"{'r-abstr_der-shift'}\", 5), (\"{'r-abstr_der-shift'}\", 4), (\"{'r-abstr_der-shift'}\", 5), (\"{'r-abstr_der-shift'}\", 2)]\n"
     ]
    }
   ],
   "source": [
    "for key in dict_with_cluster_sem_correspondence:\n",
    "    print(list(dict_with_cluster_sem_correspondence[key])[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
